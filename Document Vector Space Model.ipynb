{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\D070678\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\D070678\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "esw = stopwords.words(\"english\")\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base knowledge:\n",
    "1. for each wikipedia term, there is a term name\n",
    "2. for each wikipedia term, its English page was crawled as source text in language a -\"en\": source_a\n",
    "3. for each wikipedia term, its German page was crawled as source text in language b -\"de\": source_b\n",
    "4. for each source page in language b -\"de\", it will be translated into language a, saved as translation_b_to_a\n",
    "5. links between source_a and source_b are to be represented by the links between source-a and translation_b_to_a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getTranslationRun Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### translate the text in the format of dictionary from src language into dest language\n",
    "\n",
    "##input:\n",
    "# text: text in dictionary format {}\n",
    "# src_b: source language b of the text \n",
    "# dest_a: target language a the text to be translated into\n",
    "# termname: term name of the text\n",
    "\n",
    "def getTranslationRun(text, src_b, dest_a, termname):\n",
    "    text = text\n",
    "    dest = dest_a\n",
    "    \n",
    "    dict_trans = defaultdict(list)\n",
    "    headline = list(text.keys())\n",
    "    for el in headline:\n",
    "        translator = Translator()\n",
    "        h = translator.translate(text = el, dest = dest)\n",
    "        t = translator.translate(text = text[el], dest = dest)\n",
    "        for m in t:\n",
    "            dict_trans[h.text].append(m.text)\n",
    "            #print(m.text)\n",
    "\n",
    "    print(dict_trans)\n",
    "    return dict_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing function\n",
    "\n",
    "def toLowerList(List):\n",
    "    for i in range(len(List)):\n",
    "        List[i] = List[i].lower()\n",
    "    return List\n",
    "\n",
    "def tokenize(file):\n",
    "    i = 0\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    for el in file:\n",
    "        file[i] = ' '.join(tokenizer.tokenize(str(el)))\n",
    "        i += 1       \n",
    "\n",
    "def removeStopwords(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([word for word in el.lower().split() if word not in esw])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "\n",
    "punctuation = punctuation + str('%.[')+str('(/')+str(');[')+str('\"),')+str(').')+str('.[')+str(',[')+str('][')+str('(\"')+str('.\"[')+str('â€”'+str('.\"'))+str('.,')\n",
    "def removePunctuation(file):\n",
    "    \n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([word for word in el.lower().split() if word not in punctuation])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "\n",
    "def stemming(file):\n",
    "    ps = PorterStemmer()\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([ps.stem(word) for word in el.lower().split() ])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "\n",
    "def removeNumbers(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        file[i] = ' '.join([word for word in el.split() if not word.isdigit()])\n",
    "        i+=1\n",
    "\n",
    "        \n",
    "def lemmatize_n(file):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([lemmatizer.lemmatize(word, pos = 'n') for word in el.lower().split() ])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def lemmatize_v(file):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([lemmatizer.lemmatize(word, pos = 'v') for word in el.lower().split() ])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "def lemmatize_a(file):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([lemmatizer.lemmatize(word, pos = 'a') for word in el.lower().split() ])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "        \n",
    "def lemmatize_r(file):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([lemmatizer.lemmatize(word, pos = 'r') for word in el.lower().split() ])\n",
    "        file[i] = el\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing: tokenize, removing stopwords, punctuation, and stemming\n",
    "\n",
    "def preprocessing(content):\n",
    "    tokenize(content)\n",
    "    removeStopwords(content)\n",
    "    removePunctuation(content)\n",
    "    removeNumbers(content)\n",
    "    ### lemmatization, NOUNs\n",
    "    lemmatize_n(content)\n",
    "    ### lemmatization, Verbs\n",
    "    lemmatize_v(content)\n",
    "    ### lemmatization, Adjactives\n",
    "    lemmatize_a(content)\n",
    "    ### lemmatization\n",
    "    lemmatize_r(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTM, Query-vector building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### build functions to generate document-term matrix\n",
    "\n",
    "## get DTM, weighted by tfidf, the sqrt of the query vector is 1\n",
    "# therefore, in the retrive phase, \n",
    "# dot product of the doc vector and query vector can be used to represent the cosine similarity\n",
    "\n",
    "# input format: train_doc_file.text\n",
    "\n",
    "def get_DTM_tfidf(file):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(file)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "    #print(vocabulary)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    \n",
    "    X_train_tfidf = CountVectorizer(vocabulary = vocabulary)\n",
    "    X_train_tfidf = X_train_tfidf.fit_transform(file)\n",
    "    \n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_tfidf)\n",
    "    return X_train_tfidf\n",
    "\n",
    "\n",
    "## get DTM, weighted by term frequency\n",
    "def get_DTM_tf(file):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(file)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "\n",
    "    X_train_tf = CountVectorizer(vocabulary = vocabulary)\n",
    "    X_train_tf = X_train_tf.fit_transform(file)\n",
    "    return X_train_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Generate query vector for each query\n",
    "\n",
    "# get_QueryVector_tfidf helps to get the tiidf weighted query vector, the sqrt of the query vector is 1\n",
    "# therefore, in the retrive phase, \n",
    "# dot product of the doc vector and query vector can be used to represent the cosine similarity\n",
    "\n",
    "# input format: train_doc_file.text, train_query_file.text\n",
    "\n",
    "\n",
    "def get_QueryVector_tfidf(queryFile, docFile):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(docFile)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "    \n",
    "    if type(queryFile) is str:\n",
    "        query_vect = []\n",
    "        query = queryFile\n",
    "        query = query.split()\n",
    "        frequency = defaultdict(int)\n",
    "        for el in vocabulary:\n",
    "            if el in query:\n",
    "                frequency[el]+= 1\n",
    "            else:\n",
    "                frequency[el] = 0\n",
    "        query_vect = list(dict(frequency).values())/np.linalg.norm(list(dict(frequency).values()))\n",
    "        \n",
    "    else:\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        \n",
    "        query_vect = CountVectorizer(analyzer = \"word\", vocabulary = vocabulary)\n",
    "        query_vect = query_vect.fit_transform(queryFile)\n",
    "        \n",
    "        query_vect = tfidf_transformer.fit_transform(query_vect)\n",
    "    return query_vect\n",
    "\n",
    "\n",
    "\n",
    "###Generate the query vector, weighted by term frequency\n",
    "def get_QueryVector(queryFile, docFile):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(docFile)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "    \n",
    "    if type(queryFile) is str:\n",
    "        query_vect = []\n",
    "        query = queryFile\n",
    "        query = query.split()\n",
    "        frequency = defaultdict(int)\n",
    "        for el in vocabulary:\n",
    "            if el in query:\n",
    "                frequency[el]+= 1\n",
    "            else:\n",
    "                frequency[el] = 0\n",
    "        query_vect = list(dict(frequency).values())\n",
    "        \n",
    "    else:\n",
    "        query_vect = CountVectorizer(analyzer = \"word\", vocabulary = vocabulary)\n",
    "        query_vect = query_vect.fit_transform(queryFile)\n",
    "        \n",
    "    return query_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cosine similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: 1* n dims sparse matrix, or single vector from query_vect/ train_tfidf matrix\n",
    "def getSquareSum(vector):\n",
    "    squaresum = 0\n",
    "    for i in range(vector.shape[1]):\n",
    "        squaresum += vector[0,i]* vector[0,i]\n",
    "        \n",
    "    return squaresum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: 1* n sparse matrix, or single vector from query_vect/ train_tfidf matrix\n",
    "# return the cosine sim of two vectors\n",
    "\n",
    "def getCosineSimilarity(query_vector, doc_vector):\n",
    "    squaresum_query = getSquareSum(query_vector)\n",
    "    squaresum_doc= getSquareSum(doc_vector)\n",
    "    if math.sqrt(squaresum_query)*math.sqrt(squaresum_doc) > 0:\n",
    "        sim = np.dot(query_vector, doc_vector.transpose())[0,0]/(math.sqrt(squaresum_query)*math.sqrt(squaresum_doc))\n",
    "        return sim\n",
    "    else:\n",
    "        return np.dot(query_vector, doc_vector.transpose())[0,0]\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getVectorSpaceModelRun Function\n",
    "\n",
    "link headlines in the source text with headline in the translation text through cosinesimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_content: is the source page data of the wikipedia item in laguage a\n",
    "# trs_content: is the translation data of the wikipedia item from language b to language a\n",
    "# we link the headlines of the wikipedia item in different languages through translation text and source text\n",
    "# further more, link of headlines is predicted through similarity between the text vectors below the headlines\n",
    "# b: source language b of the text \n",
    "# a: target language a the text to be translated into\n",
    "# termname: term name of the text\n",
    "\n",
    "# return: RUN file for Trec_eval\n",
    "\n",
    "def getDocumentVectorSpaceModelRun(trs_content, src_content, src_b, dest_a, termname):    \n",
    "    ### get the similarity of query with each doc leader\n",
    "    queryVector = get_QueryVector_tfidf(trs_content, src_content)\n",
    "    src_DTM = get_DTM_tfidf(src_content)\n",
    "    \n",
    "    result = []\n",
    "    for j in range(queryVector.shape[0]):\n",
    "        \n",
    "        sims = []\n",
    "        results_ID = []\n",
    "        results = []\n",
    "        for i in range(src_DTM.shape[0]):\n",
    "            s = getCosineSimilarity(queryVector[j], src_DTM[i,])\n",
    "            #if type(s)is not None:\n",
    "            sims.append(s)\n",
    "\n",
    "        \n",
    "        for x in range(len(sims)):\n",
    "            if sims[x] >0:\n",
    "                results_ID.append(x)\n",
    "                results.append(sims[x])\n",
    "        \n",
    "        #print(results, results_ID)\n",
    "        \n",
    "        if len(results) >0 and len(results_ID)>0:\n",
    "            results, results_ID= zip(*sorted(zip(results, results_ID), reverse=True))      \n",
    "       \n",
    "        \n",
    "        for m in range(len(results)):\n",
    "            result.append([src_b+'_'+termname+\"_h\"+str(j), 0, dest_a+'_'+termname+'_h'+str(results_ID[m]), m, results[m], 'cosinesims'])\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    if len(trs_content[0].split()) > 10 and len(src_content[0].split())>10 :\n",
    "        df.to_csv(termname+'_de_en_text_VSM.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    elif len(trs_content[0].split()) < 10 and len(src_content[0].split())<10:\n",
    "        df.to_csv(termname+'_de_en_headline_VSM.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    elif len(trs_content[0].split()) < 10 and len(src_content[0].split())>10:\n",
    "        df.to_csv(termname+'_de_en_headline_text_VSM.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    else:\n",
    "        df.to_csv(termname+'_de_en_text_headline_VSM.txt', header=None, index=None, sep=' ', mode='a')     \n",
    "    \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Functions Run!\n",
    "1. data loading as source_a and source_b\n",
    "2. translating source_b into transaltion_b_to_a\n",
    "3. data preprocessing \n",
    "4. make baseline run(headlines)\n",
    "5. make cross link run(contexts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading source data\n",
    "with open('/Users/D070678/Documents/Others/MasterThesis/DataCrawling/source_de_Italy.json') as json_data:\n",
    "    source_de = json.load(json_data)\n",
    "\n",
    "with open('/Users/D070678/Documents/Others/MasterThesis/DataCrawling/source_en_Italy.json') as json_data:\n",
    "    source_en = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fbab5b738096>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#source_de_Barack_Obama: dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtranslation_de_to_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetTranslationRun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_de\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'de'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Italy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-08d88a1e4cde>\u001b[0m in \u001b[0;36mgetTranslationRun\u001b[1;34m(text, src_b, dest_a, termname)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheadline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtranslator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\googletrans-2.3.0-py3.6.egg\\googletrans\\client.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, text, dest, src)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0morigin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# this code will be updated when the format is changed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\googletrans-2.3.0-py3.6.egg\\googletrans\\client.py\u001b[0m in \u001b[0;36m_translate\u001b[1;34m(self, text, dest, src)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\googletrans-2.3.0-py3.6.egg\\googletrans\\utils.py\u001b[0m in \u001b[0;36mformat_json\u001b[1;34m(original)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlegacy_format_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\googletrans-2.3.0-py3.6.egg\\googletrans\\utils.py\u001b[0m in \u001b[0;36mlegacy_format_json\u001b[1;34m(original)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnxt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \"\"\"\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "## input: \n",
    "#source_de_Barack_Obama: dict\n",
    "\n",
    "translation_de_to_en = getTranslationRun(source_de, 'de', 'en', 'Italy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename\n",
    "\n",
    "source_a = source_en\n",
    "source_b = source_de\n",
    "#with open('/Users/hailianhou/Desktop/MasterThesis/FinalCode/translation_de_to_en_Barack_Obama.json') as json_data:\n",
    "    #translation_b_to_a = json.load(json_data)\n",
    "translation_b_to_a = translation_de_to_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact headlies\n",
    "src_headline = list(source_a.keys())\n",
    "trs_headline = list(translation_b_to_a .keys())\n",
    "\n",
    "#extract context\n",
    "src_content = []\n",
    "trs_content = []\n",
    "for el in src_headline:\n",
    "    src_content.append(''.join(source_a[el]))\n",
    "for el in trs_headline:\n",
    "    trs_content.append(''.join(translation_b_to_a[el]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessing(src_content)\n",
    "preprocessing(trs_content)\n",
    "\n",
    "preprocessing(src_headline)\n",
    "preprocessing(trs_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0  1            2  3         4           5\n",
      "0    en_China_h0  0  de_China_h6  0  0.434614  cosinesims\n",
      "1    en_China_h0  0  de_China_h4  1  0.420554  cosinesims\n",
      "2    en_China_h0  0  de_China_h0  2  0.390972  cosinesims\n",
      "3    en_China_h0  0  de_China_h3  3  0.315599  cosinesims\n",
      "4    en_China_h0  0  de_China_h1  4  0.304167  cosinesims\n",
      "5    en_China_h0  0  de_China_h2  5  0.285066  cosinesims\n",
      "6    en_China_h0  0  de_China_h8  6  0.249078  cosinesims\n",
      "7    en_China_h0  0  de_China_h7  7  0.240203  cosinesims\n",
      "8    en_China_h0  0  de_China_h5  8  0.232270  cosinesims\n",
      "9    en_China_h1  0  de_China_h0  0  0.244568  cosinesims\n",
      "10   en_China_h1  0  de_China_h4  1  0.240183  cosinesims\n",
      "11   en_China_h1  0  de_China_h6  2  0.212020  cosinesims\n",
      "12   en_China_h1  0  de_China_h8  3  0.211630  cosinesims\n",
      "13   en_China_h1  0  de_China_h3  4  0.189849  cosinesims\n",
      "14   en_China_h1  0  de_China_h2  5  0.151710  cosinesims\n",
      "15   en_China_h1  0  de_China_h5  6  0.149911  cosinesims\n",
      "16   en_China_h1  0  de_China_h1  7  0.146103  cosinesims\n",
      "17   en_China_h1  0  de_China_h7  8  0.109653  cosinesims\n",
      "18   en_China_h2  0  de_China_h1  0  0.525181  cosinesims\n",
      "19   en_China_h2  0  de_China_h4  1  0.394054  cosinesims\n",
      "20   en_China_h2  0  de_China_h6  2  0.332707  cosinesims\n",
      "21   en_China_h2  0  de_China_h3  3  0.314213  cosinesims\n",
      "22   en_China_h2  0  de_China_h8  4  0.256873  cosinesims\n",
      "23   en_China_h2  0  de_China_h5  5  0.241152  cosinesims\n",
      "24   en_China_h2  0  de_China_h0  6  0.206087  cosinesims\n",
      "25   en_China_h2  0  de_China_h2  7  0.197771  cosinesims\n",
      "26   en_China_h2  0  de_China_h7  8  0.192304  cosinesims\n",
      "27   en_China_h3  0  de_China_h2  0  0.606450  cosinesims\n",
      "28   en_China_h3  0  de_China_h6  1  0.411780  cosinesims\n",
      "29   en_China_h3  0  de_China_h4  2  0.294062  cosinesims\n",
      "..           ... ..          ... ..       ...         ...\n",
      "69   en_China_h7  0  de_China_h0  6  0.191717  cosinesims\n",
      "70   en_China_h7  0  de_China_h2  7  0.186533  cosinesims\n",
      "71   en_China_h7  0  de_China_h7  8  0.178339  cosinesims\n",
      "72   en_China_h8  0  de_China_h7  0  0.540503  cosinesims\n",
      "73   en_China_h8  0  de_China_h6  1  0.398575  cosinesims\n",
      "74   en_China_h8  0  de_China_h3  2  0.273895  cosinesims\n",
      "75   en_China_h8  0  de_China_h2  3  0.268744  cosinesims\n",
      "76   en_China_h8  0  de_China_h8  4  0.258261  cosinesims\n",
      "77   en_China_h8  0  de_China_h4  5  0.247446  cosinesims\n",
      "78   en_China_h8  0  de_China_h0  6  0.232704  cosinesims\n",
      "79   en_China_h8  0  de_China_h5  7  0.221478  cosinesims\n",
      "80   en_China_h8  0  de_China_h1  8  0.161665  cosinesims\n",
      "81   en_China_h9  0  de_China_h3  0  0.632770  cosinesims\n",
      "82   en_China_h9  0  de_China_h5  1  0.492964  cosinesims\n",
      "83   en_China_h9  0  de_China_h6  2  0.328944  cosinesims\n",
      "84   en_China_h9  0  de_China_h4  3  0.307224  cosinesims\n",
      "85   en_China_h9  0  de_China_h8  4  0.253418  cosinesims\n",
      "86   en_China_h9  0  de_China_h2  5  0.222142  cosinesims\n",
      "87   en_China_h9  0  de_China_h1  6  0.215559  cosinesims\n",
      "88   en_China_h9  0  de_China_h7  7  0.202390  cosinesims\n",
      "89   en_China_h9  0  de_China_h0  8  0.196359  cosinesims\n",
      "90  en_China_h10  0  de_China_h8  0  0.485209  cosinesims\n",
      "91  en_China_h10  0  de_China_h6  1  0.286850  cosinesims\n",
      "92  en_China_h10  0  de_China_h3  2  0.258226  cosinesims\n",
      "93  en_China_h10  0  de_China_h4  3  0.257187  cosinesims\n",
      "94  en_China_h10  0  de_China_h5  4  0.243011  cosinesims\n",
      "95  en_China_h10  0  de_China_h1  5  0.196006  cosinesims\n",
      "96  en_China_h10  0  de_China_h2  6  0.192872  cosinesims\n",
      "97  en_China_h10  0  de_China_h7  7  0.159540  cosinesims\n",
      "98  en_China_h10  0  de_China_h0  8  0.148343  cosinesims\n",
      "\n",
      "[99 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# predict headline links through context--context\n",
    "\n",
    "getDocumentVectorSpaceModelRun(src_content, trs_content,  'en', 'de', 'China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0  1            2  3        4           5\n",
      "0   en_China_h0  0  de_China_h0  0  0.57735  cosinesims\n",
      "1   en_China_h2  0  de_China_h1  0  1.00000  cosinesims\n",
      "2   en_China_h3  0  de_China_h2  0  1.00000  cosinesims\n",
      "3   en_China_h6  0  de_China_h6  0  1.00000  cosinesims\n",
      "4   en_China_h8  0  de_China_h7  0  1.00000  cosinesims\n",
      "5  en_China_h10  0  de_China_h8  0  1.00000  cosinesims\n"
     ]
    }
   ],
   "source": [
    "# predict headline links through headline--headline\n",
    "\n",
    "getDocumentVectorSpaceModelRun(src_headline, trs_headline,'en', 'de', 'China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0  1            2  3         4           5\n",
      "0    en_China_h0  0  de_China_h0  0  0.705477  cosinesims\n",
      "1    en_China_h0  0  de_China_h6  1  0.283767  cosinesims\n",
      "2    en_China_h0  0  de_China_h4  2  0.170788  cosinesims\n",
      "3    en_China_h0  0  de_China_h8  3  0.094589  cosinesims\n",
      "4    en_China_h0  0  de_China_h3  4  0.070885  cosinesims\n",
      "5    en_China_h1  0  de_China_h0  0  0.820019  cosinesims\n",
      "6    en_China_h2  0  de_China_h0  0  0.677936  cosinesims\n",
      "7    en_China_h2  0  de_China_h6  1  0.310062  cosinesims\n",
      "8    en_China_h2  0  de_China_h4  2  0.244929  cosinesims\n",
      "9    en_China_h2  0  de_China_h8  3  0.155031  cosinesims\n",
      "10   en_China_h2  0  de_China_h3  4  0.116179  cosinesims\n",
      "11   en_China_h2  0  de_China_h1  5  0.077515  cosinesims\n",
      "12   en_China_h3  0  de_China_h0  0  0.618631  cosinesims\n",
      "13   en_China_h3  0  de_China_h7  1  0.042708  cosinesims\n",
      "14   en_China_h3  0  de_China_h3  2  0.028602  cosinesims\n",
      "15   en_China_h4  0  de_China_h0  0  0.746834  cosinesims\n",
      "16   en_China_h4  0  de_China_h4  1  0.327421  cosinesims\n",
      "17   en_China_h4  0  de_China_h6  2  0.111593  cosinesims\n",
      "18   en_China_h4  0  de_China_h3  3  0.041814  cosinesims\n",
      "19   en_China_h5  0  de_China_h0  0  0.665441  cosinesims\n",
      "20   en_China_h6  0  de_China_h0  0  0.563648  cosinesims\n",
      "21   en_China_h6  0  de_China_h6  1  0.432208  cosinesims\n",
      "22   en_China_h6  0  de_China_h7  2  0.085347  cosinesims\n",
      "23   en_China_h6  0  de_China_h1  3  0.025424  cosinesims\n",
      "24   en_China_h6  0  de_China_h3  4  0.019053  cosinesims\n",
      "25   en_China_h7  0  de_China_h0  0  0.575919  cosinesims\n",
      "26   en_China_h7  0  de_China_h4  1  0.070373  cosinesims\n",
      "27   en_China_h8  0  de_China_h0  0  0.571023  cosinesims\n",
      "28   en_China_h8  0  de_China_h3  1  0.118303  cosinesims\n",
      "29   en_China_h8  0  de_China_h7  2  0.088324  cosinesims\n",
      "30   en_China_h9  0  de_China_h3  0  0.704764  cosinesims\n",
      "31   en_China_h9  0  de_China_h0  1  0.506949  cosinesims\n",
      "32   en_China_h9  0  de_China_h8  2  0.058778  cosinesims\n",
      "33   en_China_h9  0  de_China_h1  3  0.029389  cosinesims\n",
      "34  en_China_h10  0  de_China_h8  0  0.754428  cosinesims\n",
      "35  en_China_h10  0  de_China_h0  1  0.425807  cosinesims\n",
      "36  en_China_h10  0  de_China_h1  2  0.232132  cosinesims\n",
      "37  en_China_h10  0  de_China_h5  3  0.086386  cosinesims\n",
      "38  en_China_h10  0  de_China_h2  4  0.086386  cosinesims\n",
      "39  en_China_h10  0  de_China_h4  5  0.052391  cosinesims\n"
     ]
    }
   ],
   "source": [
    "# predict headline links through headline--content\n",
    "\n",
    "getDocumentVectorSpaceModelRun(src_content, trs_headline, 'en', 'de', 'China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0  1            2  3         4           5\n",
      "0    en_China_h0  0  de_China_h6  0  0.396956  cosinesims\n",
      "1    en_China_h0  0  de_China_h4  1  0.309110  cosinesims\n",
      "2    en_China_h0  0  de_China_h2  2  0.295799  cosinesims\n",
      "3    en_China_h0  0  de_China_h0  3  0.281084  cosinesims\n",
      "4    en_China_h0  0  de_China_h3  4  0.249949  cosinesims\n",
      "5    en_China_h0  0  de_China_h7  5  0.236132  cosinesims\n",
      "6    en_China_h0  0  de_China_h8  6  0.231211  cosinesims\n",
      "7    en_China_h0  0  de_China_h5  7  0.210044  cosinesims\n",
      "8    en_China_h0  0  de_China_h1  8  0.150412  cosinesims\n",
      "9    en_China_h1  0  de_China_h0  0  0.134659  cosinesims\n",
      "10   en_China_h1  0  de_China_h1  1  0.016013  cosinesims\n",
      "11   en_China_h1  0  de_China_h3  2  0.014514  cosinesims\n",
      "12   en_China_h2  0  de_China_h8  0  0.034853  cosinesims\n",
      "13   en_China_h2  0  de_China_h1  1  0.022673  cosinesims\n",
      "14   en_China_h2  0  de_China_h5  2  0.014248  cosinesims\n",
      "15   en_China_h2  0  de_China_h3  3  0.010276  cosinesims\n",
      "16   en_China_h2  0  de_China_h4  4  0.009531  cosinesims\n",
      "17   en_China_h2  0  de_China_h6  5  0.007086  cosinesims\n",
      "18   en_China_h3  0  de_China_h5  0  0.023147  cosinesims\n",
      "19   en_China_h3  0  de_China_h2  1  0.014487  cosinesims\n",
      "20   en_China_h4  0  de_China_h4  0  0.009166  cosinesims\n",
      "21   en_China_h5  0  de_China_h4  0  0.094237  cosinesims\n",
      "22   en_China_h5  0  de_China_h1  1  0.080065  cosinesims\n",
      "23   en_China_h5  0  de_China_h6  2  0.010009  cosinesims\n",
      "24   en_China_h6  0  de_China_h6  0  0.197280  cosinesims\n",
      "25   en_China_h6  0  de_China_h1  1  0.063124  cosinesims\n",
      "26   en_China_h6  0  de_China_h0  2  0.053084  cosinesims\n",
      "27   en_China_h6  0  de_China_h4  3  0.031842  cosinesims\n",
      "28   en_China_h6  0  de_China_h7  4  0.011508  cosinesims\n",
      "29   en_China_h7  0  de_China_h5  0  0.044829  cosinesims\n",
      "30   en_China_h7  0  de_China_h8  1  0.017405  cosinesims\n",
      "31   en_China_h7  0  de_China_h6  2  0.016280  cosinesims\n",
      "32   en_China_h7  0  de_China_h3  3  0.010263  cosinesims\n",
      "33   en_China_h8  0  de_China_h7  0  0.051588  cosinesims\n",
      "34   en_China_h8  0  de_China_h4  1  0.017842  cosinesims\n",
      "35   en_China_h8  0  de_China_h3  2  0.012824  cosinesims\n",
      "36   en_China_h8  0  de_China_h6  3  0.008843  cosinesims\n",
      "37  en_China_h10  0  de_China_h8  0  0.056620  cosinesims\n",
      "38  en_China_h10  0  de_China_h3  1  0.050080  cosinesims\n"
     ]
    }
   ],
   "source": [
    "# predict headline links through content--headline\n",
    "\n",
    "getDocumentVectorSpaceModelRun( src_headline, trs_content, 'en', 'de', 'China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
