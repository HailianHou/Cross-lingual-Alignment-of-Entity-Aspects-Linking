{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hailianhou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hailianhou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "esw = stopwords.words(\"english\")+['edit']\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base knowledge:\n",
    "1. for each wikipedia term, there is a term name\n",
    "2. for each wikipedia term, its English page was crawled as source text in language a -\"en\": source_a\n",
    "3. for each wikipedia term, its German page was crawled as source text in language b -\"de\": source_b\n",
    "4. for each source page in language b -\"de\", it will be translated into language a, saved as translation_b_to_a\n",
    "5. links between source_a and source_b are to be represented by the links between source-a and translation_b_to_a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getTranslationRun Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### translate the text in the format of dictionary from src language into dest language\n",
    "\n",
    "##input:\n",
    "# text: text in dictionary format {}\n",
    "# la1: source language b of the text \n",
    "# la2: target language a the text to be translated into\n",
    "# termname: term name of the text\n",
    "\n",
    "def getTranslationRun(text, src_b, dest_a, termname):\n",
    "    text = text\n",
    "    dest = dest_a\n",
    "    translator = Translator()\n",
    "    dict_trans = defaultdict(list)\n",
    "    headline = list(text.keys())\n",
    "    for el in headline:\n",
    "        h = translator.translate(text = el, dest = dest)\n",
    "        t = translator.translate(text = text[el], dest = dest)\n",
    "        for m in t:\n",
    "            dict_trans[h.text].append(m.text)\n",
    "            #print(m.text)\n",
    "\n",
    "    print(dict_trans)\n",
    "    return dict_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data preprocessing function\n",
    "\n",
    "def toLowerList(List):\n",
    "    for i in range(len(List)):\n",
    "        List[i] = List[i].lower()\n",
    "    return List\n",
    "\n",
    "def tokenizeDVSM(file):\n",
    "    i = 0\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    for el in file:\n",
    "        file[i] = ' '.join(tokenizer.tokenize(str(el)))\n",
    "        i += 1       \n",
    "\n",
    "def removeStopwords(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([word for word in el.split() if word not in esw])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "\n",
    "punctuation = punctuation + str('%.[')+str('(/')+str(');[')+str('\"),')+str(').')+str('.[')+str(',[')+str('][')+str('(\"')+str('.\"[')+str('â€”'+str('.\"'))+str('.,')\n",
    "def removePunctuation(file):\n",
    "    \n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([word for word in el.split() if word not in punctuation])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "\n",
    "def stemming(file):\n",
    "    ps = PorterStemmer()\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([ps.stem(word) for word in el.lower().split() ])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "\n",
    "def removeNumbers(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        file[i] = ' '.join([word for word in el.split() if not word.isdigit()])\n",
    "        i+=1\n",
    "\n",
    "        \n",
    "def lemmatize_n(file):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([lemmatizer.lemmatize(word, pos = 'n') for word in el.lower().split() ])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def lemmatize_v(file):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([lemmatizer.lemmatize(word, pos = 'v') for word in el.lower().split() ])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "def lemmatize_a(file):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([lemmatizer.lemmatize(word, pos = 'a') for word in el.lower().split() ])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "        \n",
    "def lemmatize_r(file):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([lemmatizer.lemmatize(word, pos = 'r') for word in el.lower().split() ])\n",
    "        file[i] = el\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing: tokenize, removing stopwords, punctuation, and stemming\n",
    "\n",
    "def preprocessing(content):\n",
    "    tokenizeDVSM(content)\n",
    "    removePunctuation(content)\n",
    "    removeStopwords(content)\n",
    "    removeNumbers(content)\n",
    "    ### lemmatization, NOUNs\n",
    "    lemmatize_n(content)\n",
    "    ### lemmatization, Verbs\n",
    "    lemmatize_v(content)\n",
    "    ### lemmatization, Adjactives\n",
    "    lemmatize_a(content)\n",
    "    ### lemmatization\n",
    "    lemmatize_r(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTM, Query-vector building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### build functions to generate document-term matrix\n",
    "\n",
    "## get DTM, weighted by tfidf, the sqrt of the query vector is 1\n",
    "# therefore, in the retrive phase, \n",
    "# dot product of the doc vector and query vector can be used to represent the cosine similarity\n",
    "\n",
    "# input format: train_doc_file.text\n",
    "\n",
    "def get_DTM_tfidf(file):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(file)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "    #print(vocabulary)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    \n",
    "    X_train_tfidf = CountVectorizer(vocabulary = vocabulary)\n",
    "    X_train_tfidf = X_train_tfidf.fit_transform(file)\n",
    "    \n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_tfidf)\n",
    "    return X_train_tfidf\n",
    "\n",
    "\n",
    "## get DTM, weighted by term frequency\n",
    "def get_DTM_tf(file):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(file)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "\n",
    "    X_train_tf = CountVectorizer(vocabulary = vocabulary)\n",
    "    X_train_tf = X_train_tf.fit_transform(file)\n",
    "    return X_train_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Generate query vector for each query\n",
    "\n",
    "# get_QueryVector_tfidf helps to get the tiidf weighted query vector, the sqrt of the query vector is 1\n",
    "# therefore, in the retrive phase, \n",
    "# dot product of the doc vector and query vector can be used to represent the cosine similarity\n",
    "\n",
    "# input format: train_doc_file.text, train_query_file.text\n",
    "\n",
    "\n",
    "def get_QueryVector_tfidf(queryFile, docFile):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(docFile)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "    \n",
    "    if type(queryFile) is str:\n",
    "        query_vect = []\n",
    "        query = queryFile\n",
    "        query = query.split()\n",
    "        frequency = defaultdict(int)\n",
    "        for el in vocabulary:\n",
    "            if el in query:\n",
    "                frequency[el]+= 1\n",
    "            else:\n",
    "                frequency[el] = 0\n",
    "        query_vect = list(dict(frequency).values())/np.linalg.norm(list(dict(frequency).values()))\n",
    "        \n",
    "    else:\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        \n",
    "        query_vect = CountVectorizer(analyzer = \"word\", vocabulary = vocabulary)\n",
    "        query_vect = query_vect.fit_transform(queryFile)\n",
    "        \n",
    "        query_vect = tfidf_transformer.fit_transform(query_vect)\n",
    "    return query_vect\n",
    "\n",
    "\n",
    "\n",
    "###Generate the query vector, weighted by term frequency\n",
    "def get_QueryVector(queryFile, docFile):\n",
    "    train_count_vect = CountVectorizer() #remove english stopwords\n",
    "    X_train_counts = train_count_vect.fit_transform(docFile)\n",
    "    vocabulary = list(train_count_vect.vocabulary_.keys())\n",
    "    \n",
    "    if type(queryFile) is str:\n",
    "        query_vect = []\n",
    "        query = queryFile\n",
    "        query = query.split()\n",
    "        frequency = defaultdict(int)\n",
    "        for el in vocabulary:\n",
    "            if el in query:\n",
    "                frequency[el]+= 1\n",
    "            else:\n",
    "                frequency[el] = 0\n",
    "        query_vect = list(dict(frequency).values())\n",
    "        \n",
    "    else:\n",
    "        query_vect = CountVectorizer(analyzer = \"word\", vocabulary = vocabulary)\n",
    "        query_vect = query_vect.fit_transform(queryFile)\n",
    "        \n",
    "    return query_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cosine similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: 1* n dims sparse matrix, or single vector from query_vect/ train_tfidf matrix\n",
    "def getSquareSum(vector):\n",
    "    squaresum = 0\n",
    "    for i in range(vector.shape[1]):\n",
    "        squaresum += vector[0,i]* vector[0,i]\n",
    "        \n",
    "    return squaresum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: 1* n sparse matrix, or single vector from query_vect/ train_tfidf matrix\n",
    "# return the cosine sim of two vectors\n",
    "\n",
    "def getCosineSimilarity(query_vector, doc_vector):\n",
    "    squaresum_query = getSquareSum(query_vector)\n",
    "    squaresum_doc= getSquareSum(doc_vector)\n",
    "    if math.sqrt(squaresum_query)*math.sqrt(squaresum_doc) > 0:\n",
    "        sim = np.dot(query_vector, doc_vector.transpose())[0,0]/(math.sqrt(squaresum_query)*math.sqrt(squaresum_doc))\n",
    "        return sim\n",
    "    else:\n",
    "        return np.dot(query_vector, doc_vector.transpose())[0,0]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getVectorSpaceModelRun Function\n",
    "\n",
    "link headlines in the source text with headline in the translation text through cosinesimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# src_content: is the source page data of the wikipedia item in laguage a\n",
    "# trs_content: is the translation data of the wikipedia item from language b to language a\n",
    "# we link the headlines of the wikipedia item in different languages through translation text and source text\n",
    "# further more, link of headlines is predicted through similarity between the text vectors below the headlines\n",
    "# b: source language b of the text \n",
    "# a: target language a the text to be translated into\n",
    "# termname: term name of the text\n",
    "\n",
    "# return: RUN file for Trec_eval\n",
    "\n",
    "def getDocumentVectorSpaceModelRun(trs_content, src_content, la1, la2, termname):    \n",
    "    ### get the similarity of query with each doc leader\n",
    "    queryVector = get_QueryVector_tfidf(trs_content, src_content)\n",
    "    src_DTM = get_DTM_tfidf(src_content)\n",
    "    \n",
    "    result = []\n",
    "    for j in range(queryVector.shape[0]):\n",
    "        \n",
    "        sims = []\n",
    "        results_ID = []\n",
    "        results = []\n",
    "        for i in range(src_DTM.shape[0]):\n",
    "            s = getCosineSimilarity(queryVector[j], src_DTM[i,])\n",
    "            #if type(s)is not None:\n",
    "            sims.append(s)\n",
    "\n",
    "        \n",
    "        for x in range(len(sims)):\n",
    "            if sims[x] >0:\n",
    "                results_ID.append(x)\n",
    "                results.append(sims[x])\n",
    "        \n",
    "        #print(results, results_ID)\n",
    "        \n",
    "        if len(results) >0 and len(results_ID)>0:\n",
    "            results, results_ID= zip(*sorted(zip(results, results_ID), reverse=True))      \n",
    "       \n",
    "        \n",
    "        for m in range(len(results)):\n",
    "            result.append([la1+'_'+termname+\"_h\"+str(j), 0, la2+'_'+termname+'_h'+str(results_ID[m]), m, results[m], 'cosinesims'])\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    if len(trs_content[0].split()) > 10 and len(src_content[0].split())>10 :\n",
    "        df.to_csv('DWSM_'+termname+'_'+la1+'_'+la2+'_text.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    elif len(trs_content[0].split()) < 10 and len(src_content[0].split())<10:\n",
    "        df.to_csv('DWSM_'+termname+'_'+la1+'_'+la2+'_headline.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    elif len(trs_content[0].split()) < 10 and len(src_content[0].split())>10:\n",
    "        df.to_csv('DWSM_'+termname+'_'+la1+'_'+la2+'_headline_text.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    else:\n",
    "        df.to_csv('DWSM_'+termname+'_'+la1+'_'+la2+'_text_headline.txt', header=None, index=None, sep=' ', mode='a')     \n",
    "    \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Functions Run!\n",
    "1. data loading as source_a and source_b\n",
    "2. translating source_b into transaltion_b_to_a\n",
    "3. data preprocessing \n",
    "4. make baseline run(headlines)\n",
    "5. make cross link run(contexts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ff1b5d83df79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# importing word vector for WVSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0men_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/hailianhou/Desktop/MasterThesis/Wiki Data/wiki-news-300d-1M.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/hailianhou/anaconda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1118\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hailianhou/anaconda/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hailianhou/anaconda/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# importing word vector for WVSM\n",
    "en_model = KeyedVectors.load_word2vec_format('/Users/hailianhou/Desktop/MasterThesis/Wiki Data/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'United_States','Germany', 'Japan','China', 'France','Italy', 'Canada', 'United_Kingdom', \n",
    "#'Asia','Europe', 'Russia','singapore','India', 'Israel','Brazil','Philippines'\n",
    "# 'New_York_City','London','Singapore','Hong_Kong','Dubai','Los_Angeles','Paris','Chicago','San_Francisco','Mumbai','Rome',\n",
    "#'Toronto','Philadelphia','Monaco','Tokyo','Amsterdam','Boston','Barcelona','Peking'\n",
    "\n",
    "entity_name = 'United_States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading source data\n",
    "with open('/Users/hailianhou/Desktop/MasterThesis/FinalCode/DataCrawling/source_zh_'+entity_name+'.json') as json_data:\n",
    "    source_zh = json.load(json_data)\n",
    "\n",
    "with open('/Users/hailianhou/Desktop/MasterThesis/FinalCode/DataCrawling/source_en_'+entity_name+'.json') as json_data:\n",
    "    source_en = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-557939e63cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#source_zh_Barack_Obama: dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtranslation_zh_to_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTranslationRun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_zh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-b919d70cda88>\u001b[0m in \u001b[0;36mgetTranslationRun\u001b[0;34m(text, src_b, dest_a, termname)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheadline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mdict_trans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hailianhou/anaconda/lib/python3.6/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hailianhou/anaconda/lib/python3.6/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hailianhou/anaconda/lib/python3.6/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hailianhou/anaconda/lib/python3.6/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mformat_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegacy_format_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hailianhou/anaconda/lib/python3.6/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mlegacy_format_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hailianhou/anaconda/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hailianhou/anaconda/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hailianhou/anaconda/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "## input: \n",
    "#source_zh_Barack_Obama: dict\n",
    "\n",
    "translation_zh_to_en = getTranslationRun(source_zh, 'zh', 'en', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translation_zh_to_en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-04b0657be42f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msource_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_zh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtranslation_b_to_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslation_zh_to_en\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'translation_zh_to_en' is not defined"
     ]
    }
   ],
   "source": [
    "#rename\n",
    "\n",
    "source_a = source_en\n",
    "source_b = source_zh\n",
    "\n",
    "translation_b_to_a = translation_zh_to_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# exact headlies\n",
    "src_headline = list(source_a.keys())\n",
    "trs_headline = list(translation_b_to_a .keys())\n",
    "\n",
    "#extract context\n",
    "src_content = []\n",
    "trs_content = []\n",
    "for el in src_headline:\n",
    "    src_content.append(''.join(source_a[el]))\n",
    "for el in trs_headline:\n",
    "    trs_content.append(''.join(translation_b_to_a[el]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessing(src_content)\n",
    "preprocessing(trs_content)\n",
    "\n",
    "preprocessing(src_headline)\n",
    "preprocessing(trs_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1                2  3         4           5\n",
      "0     en_Amsterdam_h0  0  de_Amsterdam_h0  0  0.538282  cosinesims\n",
      "1     en_Amsterdam_h0  0  de_Amsterdam_h7  1  0.528043  cosinesims\n",
      "2     en_Amsterdam_h0  0  de_Amsterdam_h3  2  0.468488  cosinesims\n",
      "3     en_Amsterdam_h0  0  de_Amsterdam_h1  3  0.333967  cosinesims\n",
      "4     en_Amsterdam_h0  0  de_Amsterdam_h6  4  0.289278  cosinesims\n",
      "5     en_Amsterdam_h0  0  de_Amsterdam_h4  5  0.271678  cosinesims\n",
      "6     en_Amsterdam_h0  0  de_Amsterdam_h5  6  0.214613  cosinesims\n",
      "7     en_Amsterdam_h0  0  de_Amsterdam_h2  7  0.154069  cosinesims\n",
      "8     en_Amsterdam_h0  0  de_Amsterdam_h8  8  0.081040  cosinesims\n",
      "9     en_Amsterdam_h1  0  de_Amsterdam_h3  0  0.206090  cosinesims\n",
      "10    en_Amsterdam_h1  0  de_Amsterdam_h0  1  0.110817  cosinesims\n",
      "11    en_Amsterdam_h1  0  de_Amsterdam_h4  2  0.071878  cosinesims\n",
      "12    en_Amsterdam_h1  0  de_Amsterdam_h5  3  0.052722  cosinesims\n",
      "13    en_Amsterdam_h1  0  de_Amsterdam_h7  4  0.051014  cosinesims\n",
      "14    en_Amsterdam_h1  0  de_Amsterdam_h6  5  0.042935  cosinesims\n",
      "15    en_Amsterdam_h1  0  de_Amsterdam_h2  6  0.042519  cosinesims\n",
      "16    en_Amsterdam_h1  0  de_Amsterdam_h8  7  0.034573  cosinesims\n",
      "17    en_Amsterdam_h2  0  de_Amsterdam_h3  0  0.601650  cosinesims\n",
      "18    en_Amsterdam_h2  0  de_Amsterdam_h7  1  0.484395  cosinesims\n",
      "19    en_Amsterdam_h2  0  de_Amsterdam_h0  2  0.435273  cosinesims\n",
      "20    en_Amsterdam_h2  0  de_Amsterdam_h1  3  0.376033  cosinesims\n",
      "21    en_Amsterdam_h2  0  de_Amsterdam_h4  4  0.301591  cosinesims\n",
      "22    en_Amsterdam_h2  0  de_Amsterdam_h6  5  0.254718  cosinesims\n",
      "23    en_Amsterdam_h2  0  de_Amsterdam_h5  6  0.236360  cosinesims\n",
      "24    en_Amsterdam_h2  0  de_Amsterdam_h2  7  0.185650  cosinesims\n",
      "25    en_Amsterdam_h2  0  de_Amsterdam_h8  8  0.058754  cosinesims\n",
      "26    en_Amsterdam_h3  0  de_Amsterdam_h0  0  0.426946  cosinesims\n",
      "27    en_Amsterdam_h3  0  de_Amsterdam_h7  1  0.422113  cosinesims\n",
      "28    en_Amsterdam_h3  0  de_Amsterdam_h2  2  0.364150  cosinesims\n",
      "29    en_Amsterdam_h3  0  de_Amsterdam_h3  3  0.320269  cosinesims\n",
      "..                ... ..              ... ..       ...         ...\n",
      "86    en_Amsterdam_h9  0  de_Amsterdam_h4  6  0.173377  cosinesims\n",
      "87    en_Amsterdam_h9  0  de_Amsterdam_h2  7  0.153359  cosinesims\n",
      "88    en_Amsterdam_h9  0  de_Amsterdam_h8  8  0.080619  cosinesims\n",
      "89   en_Amsterdam_h10  0  de_Amsterdam_h7  0  0.238567  cosinesims\n",
      "90   en_Amsterdam_h10  0  de_Amsterdam_h1  1  0.227695  cosinesims\n",
      "91   en_Amsterdam_h10  0  de_Amsterdam_h0  2  0.169320  cosinesims\n",
      "92   en_Amsterdam_h10  0  de_Amsterdam_h3  3  0.157688  cosinesims\n",
      "93   en_Amsterdam_h10  0  de_Amsterdam_h5  4  0.142314  cosinesims\n",
      "94   en_Amsterdam_h10  0  de_Amsterdam_h6  5  0.135178  cosinesims\n",
      "95   en_Amsterdam_h10  0  de_Amsterdam_h4  6  0.108915  cosinesims\n",
      "96   en_Amsterdam_h10  0  de_Amsterdam_h2  7  0.060019  cosinesims\n",
      "97   en_Amsterdam_h10  0  de_Amsterdam_h8  8  0.039269  cosinesims\n",
      "98   en_Amsterdam_h11  0  de_Amsterdam_h7  0  0.398228  cosinesims\n",
      "99   en_Amsterdam_h11  0  de_Amsterdam_h0  1  0.265304  cosinesims\n",
      "100  en_Amsterdam_h11  0  de_Amsterdam_h3  2  0.250537  cosinesims\n",
      "101  en_Amsterdam_h11  0  de_Amsterdam_h1  3  0.240050  cosinesims\n",
      "102  en_Amsterdam_h11  0  de_Amsterdam_h6  4  0.188173  cosinesims\n",
      "103  en_Amsterdam_h11  0  de_Amsterdam_h4  5  0.187234  cosinesims\n",
      "104  en_Amsterdam_h11  0  de_Amsterdam_h5  6  0.122766  cosinesims\n",
      "105  en_Amsterdam_h11  0  de_Amsterdam_h2  7  0.101124  cosinesims\n",
      "106  en_Amsterdam_h11  0  de_Amsterdam_h8  8  0.042072  cosinesims\n",
      "107  en_Amsterdam_h12  0  de_Amsterdam_h3  0  0.178789  cosinesims\n",
      "108  en_Amsterdam_h12  0  de_Amsterdam_h7  1  0.159690  cosinesims\n",
      "109  en_Amsterdam_h12  0  de_Amsterdam_h1  2  0.157446  cosinesims\n",
      "110  en_Amsterdam_h12  0  de_Amsterdam_h5  3  0.148759  cosinesims\n",
      "111  en_Amsterdam_h12  0  de_Amsterdam_h0  4  0.132413  cosinesims\n",
      "112  en_Amsterdam_h12  0  de_Amsterdam_h6  5  0.127247  cosinesims\n",
      "113  en_Amsterdam_h12  0  de_Amsterdam_h4  6  0.086685  cosinesims\n",
      "114  en_Amsterdam_h12  0  de_Amsterdam_h2  7  0.040416  cosinesims\n",
      "115  en_Amsterdam_h12  0  de_Amsterdam_h8  8  0.018727  cosinesims\n",
      "\n",
      "[116 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# English-German aspects links through context--context \n",
    "\n",
    "getDocumentVectorSpaceModelRun(src_content, trs_content, 'en', 'zh', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0  1                2  3         4           5\n",
      "0  en_Amsterdam_h0  0  de_Amsterdam_h0  0  1.000000  cosinesims\n",
      "1  en_Amsterdam_h2  0  de_Amsterdam_h3  0  0.920794  cosinesims\n",
      "2  en_Amsterdam_h3  0  de_Amsterdam_h2  0  0.920794  cosinesims\n",
      "3  en_Amsterdam_h5  0  de_Amsterdam_h5  0  0.920794  cosinesims\n",
      "4  en_Amsterdam_h6  0  de_Amsterdam_h7  0  0.677373  cosinesims\n",
      "5  en_Amsterdam_h7  0  de_Amsterdam_h6  0  0.920794  cosinesims\n",
      "6  en_Amsterdam_h8  0  de_Amsterdam_h4  0  0.920794  cosinesims\n"
     ]
    }
   ],
   "source": [
    "# English-German aspects linking through headline--headline\n",
    "\n",
    "getDocumentVectorSpaceModelRun(src_headline, trs_headline, 'en', 'zh', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0  1                2  3         4           5\n",
      "0    en_Amsterdam_h0  0  de_Amsterdam_h0  0  0.988362  cosinesims\n",
      "1    en_Amsterdam_h0  0  de_Amsterdam_h7  1  0.083340  cosinesims\n",
      "2    en_Amsterdam_h0  0  de_Amsterdam_h1  2  0.080272  cosinesims\n",
      "3    en_Amsterdam_h2  0  de_Amsterdam_h0  0  0.998465  cosinesims\n",
      "4    en_Amsterdam_h2  0  de_Amsterdam_h1  1  0.049702  cosinesims\n",
      "5    en_Amsterdam_h3  0  de_Amsterdam_h0  0  1.000000  cosinesims\n",
      "6    en_Amsterdam_h4  0  de_Amsterdam_h0  0  0.998861  cosinesims\n",
      "7    en_Amsterdam_h4  0  de_Amsterdam_h6  1  0.043939  cosinesims\n",
      "8    en_Amsterdam_h5  0  de_Amsterdam_h0  0  0.893462  cosinesims\n",
      "9    en_Amsterdam_h5  0  de_Amsterdam_h5  1  0.347134  cosinesims\n",
      "10   en_Amsterdam_h5  0  de_Amsterdam_h1  2  0.162431  cosinesims\n",
      "11   en_Amsterdam_h5  0  de_Amsterdam_h3  3  0.074839  cosinesims\n",
      "12   en_Amsterdam_h5  0  de_Amsterdam_h6  4  0.054419  cosinesims\n",
      "13   en_Amsterdam_h6  0  de_Amsterdam_h0  0  0.945846  cosinesims\n",
      "14   en_Amsterdam_h6  0  de_Amsterdam_h1  1  0.292262  cosinesims\n",
      "15   en_Amsterdam_h6  0  de_Amsterdam_h7  2  0.065081  cosinesims\n",
      "16   en_Amsterdam_h7  0  de_Amsterdam_h0  0  0.998861  cosinesims\n",
      "17   en_Amsterdam_h7  0  de_Amsterdam_h6  1  0.043939  cosinesims\n",
      "18   en_Amsterdam_h8  0  de_Amsterdam_h0  0  0.871465  cosinesims\n",
      "19   en_Amsterdam_h8  0  de_Amsterdam_h1  1  0.293553  cosinesims\n",
      "20   en_Amsterdam_h8  0  de_Amsterdam_h7  2  0.023270  cosinesims\n",
      "21   en_Amsterdam_h9  0  de_Amsterdam_h0  0  0.992486  cosinesims\n",
      "22   en_Amsterdam_h9  0  de_Amsterdam_h6  1  0.071442  cosinesims\n",
      "23   en_Amsterdam_h9  0  de_Amsterdam_h1  2  0.028298  cosinesims\n",
      "24  en_Amsterdam_h10  0  de_Amsterdam_h0  0  0.931669  cosinesims\n",
      "25  en_Amsterdam_h10  0  de_Amsterdam_h3  1  0.304353  cosinesims\n",
      "26  en_Amsterdam_h10  0  de_Amsterdam_h4  2  0.117643  cosinesims\n",
      "27  en_Amsterdam_h10  0  de_Amsterdam_h6  3  0.073771  cosinesims\n",
      "28  en_Amsterdam_h11  0  de_Amsterdam_h0  0  1.000000  cosinesims\n",
      "29  en_Amsterdam_h12  0  de_Amsterdam_h0  0  1.000000  cosinesims\n"
     ]
    }
   ],
   "source": [
    "# English-German aspects linking through headline--content\n",
    "\n",
    "getDocumentVectorSpaceModelRun(src_content, trs_headline,  'en', 'zh', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0  1                2  3         4           5\n",
      "0    en_Amsterdam_h0  0  de_Amsterdam_h1  0  0.632870  cosinesims\n",
      "1    en_Amsterdam_h0  0  de_Amsterdam_h0  1  0.407440  cosinesims\n",
      "2    en_Amsterdam_h0  0  de_Amsterdam_h7  2  0.406263  cosinesims\n",
      "3    en_Amsterdam_h0  0  de_Amsterdam_h6  3  0.276485  cosinesims\n",
      "4    en_Amsterdam_h0  0  de_Amsterdam_h3  4  0.209986  cosinesims\n",
      "5    en_Amsterdam_h0  0  de_Amsterdam_h5  5  0.172987  cosinesims\n",
      "6    en_Amsterdam_h0  0  de_Amsterdam_h4  6  0.124671  cosinesims\n",
      "7    en_Amsterdam_h0  0  de_Amsterdam_h8  7  0.084833  cosinesims\n",
      "8    en_Amsterdam_h0  0  de_Amsterdam_h2  8  0.072041  cosinesims\n",
      "9    en_Amsterdam_h2  0  de_Amsterdam_h5  0  0.056425  cosinesims\n",
      "10   en_Amsterdam_h5  0  de_Amsterdam_h5  0  0.047657  cosinesims\n",
      "11   en_Amsterdam_h5  0  de_Amsterdam_h3  1  0.014463  cosinesims\n",
      "12   en_Amsterdam_h7  0  de_Amsterdam_h8  0  0.221367  cosinesims\n",
      "13   en_Amsterdam_h8  0  de_Amsterdam_h6  0  0.072147  cosinesims\n",
      "14   en_Amsterdam_h9  0  de_Amsterdam_h5  0  0.109835  cosinesims\n",
      "15   en_Amsterdam_h9  0  de_Amsterdam_h7  1  0.041689  cosinesims\n",
      "16   en_Amsterdam_h9  0  de_Amsterdam_h3  2  0.022221  cosinesims\n",
      "17   en_Amsterdam_h9  0  de_Amsterdam_h4  3  0.021109  cosinesims\n",
      "18  en_Amsterdam_h11  0  de_Amsterdam_h7  0  0.032125  cosinesims\n",
      "19  en_Amsterdam_h12  0  de_Amsterdam_h5  0  0.207184  cosinesims\n",
      "20  en_Amsterdam_h12  0  de_Amsterdam_h3  1  0.037725  cosinesims\n",
      "21  en_Amsterdam_h12  0  de_Amsterdam_h7  2  0.023591  cosinesims\n"
     ]
    }
   ],
   "source": [
    "# English-German aspects linking through content--headline\n",
    "\n",
    "getDocumentVectorSpaceModelRun(src_headline, trs_content,  'en', 'zh', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vector Space Model with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hailianhou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hailianhou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "esw = stopwords.words(\"english\")+['edit']\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traditionalToSimplified(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        file[i] = ''.join(HanziConv.toSimplified(el))\n",
    "        i += 1\n",
    "        \n",
    "def chineseTokenize(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        file[i] = ' '.join(jieba.cut(el, cut_all=False, HMM=True))\n",
    "        i += 1\n",
    "\n",
    "        \n",
    "from nltk.tokenize import MWETokenizer\n",
    "def MWEtokenize(el):\n",
    "    i = 0\n",
    "    tokenizer = MWETokenizer()\n",
    "    \n",
    "    el = tokenizer.tokenize(el.split())\n",
    "    return el\n",
    "\n",
    "punctuation = punctuation + str('ï¼›')+  str(\"ï¼šã€Šã€‹ã€Œ ã€â€œâ€[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰]+\")+str('ç¼–è¾‘')+str('%.[')+str('(/')+str(');[')+str('\"),')+str(').')+str('.[')+str(',[')+str('][')+str('(\"')+str('.\"[')+str('â€”'+str('.\"'))+str('.,')        \n",
    "def tokenize(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        tokenizerOne = WordPunctTokenizer()\n",
    "        el = tokenizerOne.tokenize(str(el.lower()))\n",
    "        el = ' '.join([word for word in el if word not in punctuation])\n",
    "        file[i] = MWEtokenize(el)\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exact headlies\n",
    "en_headline = list(source_a.keys())\n",
    "zh_headline = list(translation_b_to_a .keys())\n",
    "\n",
    "#extract context\n",
    "en_content = []\n",
    "zh_content = []\n",
    "for el in en_headline:\n",
    "    en_content.append(''.join(source_a[el]))\n",
    "for el in zh_headline:\n",
    "    zh_content.append(''.join(translation_b_to_a[el]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing for en and de\n",
    "tokenize(en_content)\n",
    "tokenize(zh_content)\n",
    "tokenize(en_headline)\n",
    "tokenize(zh_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['amsterdam'],\n",
       " ['etymology', 'edit'],\n",
       " ['history', 'edit'],\n",
       " ['geography', 'edit'],\n",
       " ['demographics', 'edit'],\n",
       " ['cityscape', 'and', 'architecture', 'edit'],\n",
       " ['economy', 'edit'],\n",
       " ['culture', 'edit'],\n",
       " ['politics', 'edit'],\n",
       " ['transport', 'edit'],\n",
       " ['education', 'edit'],\n",
       " ['media', 'edit'],\n",
       " ['housing', 'edit']]"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building the word2vect model for the documents corpus\n",
    "\n",
    "model = Word2Vec(en_content, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDocVectors(content):\n",
    "    \n",
    "    doc_vectors = list()\n",
    "    for i in range(len(content)):\n",
    "        \n",
    "        vectorSum = [0]*300\n",
    "        l = 0\n",
    "        for el in content[i]:\n",
    "            if el in list(model.wv.vocab):\n",
    "                # adding the word vectors togenther by their dimensions:\n",
    "                vectorSum = list(map(add, list(model.wv[el]), vectorSum))\n",
    "                l+=1\n",
    "        \n",
    "        for m in range(len(vectorSum)):\n",
    "            if vectorSum[m] != 0:\n",
    "                ###anormalize the vector sum:\n",
    "                vectorSum[m] = float(vectorSum[m])/l \n",
    "            else:\n",
    "                vectorSum[m] = vectorSum[m]\n",
    "          \n",
    "        doc_vectors.append(vectorSum)\n",
    "\n",
    "    return doc_vectors \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: 1* n dims sparse matrix, or single vector from query_vect/ train_tfidf matrix\n",
    "def getSquareSumWVSM(vector):\n",
    "    squaresum = 0\n",
    "    for i in range(len(vector)):\n",
    "        squaresum += vector[i]* vector[i]\n",
    "        \n",
    "    return squaresum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: 1* n sparse matrix, or single vector from query_vect/ train_tfidf matrix\n",
    "# return the cosine sim of two vectors\n",
    "\n",
    "def getCosineSimilarityWVSM(query_vector, doc_vector):\n",
    "    squaresum_query = getSquareSumWVSM(query_vector)\n",
    "    squaresum_doc= getSquareSumWVSM(doc_vector)\n",
    "    if math.sqrt(squaresum_query)*math.sqrt(squaresum_doc) > 0:\n",
    "        sim = np.dot(query_vector, doc_vector)/(math.sqrt(squaresum_query)*math.sqrt(squaresum_doc))\n",
    "        return sim\n",
    "    else:\n",
    "        return np.dot(query_vector, doc_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# src_content: is the source page data of the wikipedia item in laguage a\n",
    "# trs_content: is the translation data of the wikipedia item from language b to language a\n",
    "# we link the headlines of the wikipedia item in different languages through translation text and source text\n",
    "# further more, link of headlines is predicted through similarity between the text vectors below the headlines\n",
    "# b: source language b of the text \n",
    "# a: target language a the text to be translated into\n",
    "# termname: term name of the text\n",
    "\n",
    "# return: RUN file for Trec_eval\n",
    "\n",
    "def getWordVectorSpaceModelRun(trs_content, src_content, la1, la2, termname):    \n",
    "    ### get the similarity of query with each doc leader\n",
    "    DV_src_content = getDocVectors(src_content)\n",
    "    DV_trs_content = getDocVectors(trs_content)\n",
    "    \n",
    "    \n",
    "    result = []\n",
    "    for j in range(len(DV_trs_content)):\n",
    "        \n",
    "        sims = []\n",
    "        results_ID = []\n",
    "        results = []\n",
    "        for i in range(len(DV_src_content)):\n",
    "            s = getCosineSimilarityWVSM(DV_trs_content[j], DV_src_content[i])\n",
    "            #if type(s)is not None:\n",
    "            sims.append(s)\n",
    "\n",
    "        \n",
    "        for x in range(len(sims)):\n",
    "            if sims[x] >0:\n",
    "                results_ID.append(x)\n",
    "                results.append(sims[x])\n",
    "        \n",
    "        #print(results, results_ID)\n",
    "        \n",
    "        if len(results) >0 and len(results_ID)>0:\n",
    "            results, results_ID= zip(*sorted(zip(results, results_ID), reverse=True))      \n",
    "       \n",
    "        \n",
    "        for m in range(len(results)):\n",
    "            result.append([la1+'_'+termname+\"_h\"+str(j), 0, la2+'_'+termname+'_h'+str(results_ID[m]), m, results[m], 'word2vector'])\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    if len(trs_content[0]) > 10 and len(src_content[0])>10 :\n",
    "        df.to_csv('WVSM_'+termname+'_'+la1+'_'+la2+'_text.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    elif len(trs_content[0]) < 10 and len(src_content[0])<10:\n",
    "        df.to_csv('WVSM_'+termname+'_'+la1+'_'+la2+'_headline.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    elif len(trs_content[0]) < 10 and len(src_content[0])>10:\n",
    "        df.to_csv('WVSM_'+termname+'_'+la1+'_'+la2+'_headline_text.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    else:\n",
    "        df.to_csv('WVSM_'+termname+'_'+la1+'_'+la2+'_text_headline.txt', header=None, index=None, sep=' ', mode='a')     \n",
    "    \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0  1                2  3         4            5\n",
      "0    en_Amsterdam_h0  0  de_Amsterdam_h0  0  1.000000  word2vector\n",
      "1    en_Amsterdam_h0  0  de_Amsterdam_h7  1  0.999977  word2vector\n",
      "2    en_Amsterdam_h0  0  de_Amsterdam_h1  2  0.999969  word2vector\n",
      "3    en_Amsterdam_h0  0  de_Amsterdam_h6  3  0.999337  word2vector\n",
      "4    en_Amsterdam_h5  0  de_Amsterdam_h7  0  1.000000  word2vector\n",
      "5    en_Amsterdam_h5  0  de_Amsterdam_h1  1  0.999979  word2vector\n",
      "6    en_Amsterdam_h5  0  de_Amsterdam_h0  2  0.999977  word2vector\n",
      "7    en_Amsterdam_h5  0  de_Amsterdam_h6  3  0.999324  word2vector\n",
      "8    en_Amsterdam_h7  0  de_Amsterdam_h6  0  1.000000  word2vector\n",
      "9    en_Amsterdam_h7  0  de_Amsterdam_h0  1  0.999337  word2vector\n",
      "10   en_Amsterdam_h7  0  de_Amsterdam_h7  2  0.999324  word2vector\n",
      "11   en_Amsterdam_h7  0  de_Amsterdam_h1  3  0.999312  word2vector\n",
      "12   en_Amsterdam_h9  0  de_Amsterdam_h0  0  0.999373  word2vector\n",
      "13   en_Amsterdam_h9  0  de_Amsterdam_h7  1  0.999362  word2vector\n",
      "14   en_Amsterdam_h9  0  de_Amsterdam_h1  2  0.999358  word2vector\n",
      "15   en_Amsterdam_h9  0  de_Amsterdam_h6  3  0.998866  word2vector\n",
      "16  en_Amsterdam_h12  0  de_Amsterdam_h0  0  0.999730  word2vector\n",
      "17  en_Amsterdam_h12  0  de_Amsterdam_h7  1  0.999726  word2vector\n",
      "18  en_Amsterdam_h12  0  de_Amsterdam_h1  2  0.999712  word2vector\n",
      "19  en_Amsterdam_h12  0  de_Amsterdam_h6  3  0.999044  word2vector\n"
     ]
    }
   ],
   "source": [
    "# Englis-German Aspects Linking Headline-Headline \n",
    "\n",
    "getWordVectorSpaceModelRun(en_headline, de_headline, 'en', 'zh', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1                2  3         4            5\n",
      "0     en_Amsterdam_h0  0  de_Amsterdam_h7  0  1.000000  word2vector\n",
      "1     en_Amsterdam_h0  0  de_Amsterdam_h4  1  1.000000  word2vector\n",
      "2     en_Amsterdam_h0  0  de_Amsterdam_h3  2  1.000000  word2vector\n",
      "3     en_Amsterdam_h0  0  de_Amsterdam_h6  3  1.000000  word2vector\n",
      "4     en_Amsterdam_h0  0  de_Amsterdam_h0  4  1.000000  word2vector\n",
      "5     en_Amsterdam_h0  0  de_Amsterdam_h2  5  1.000000  word2vector\n",
      "6     en_Amsterdam_h0  0  de_Amsterdam_h5  6  1.000000  word2vector\n",
      "7     en_Amsterdam_h0  0  de_Amsterdam_h1  7  0.999998  word2vector\n",
      "8     en_Amsterdam_h0  0  de_Amsterdam_h8  8  0.999998  word2vector\n",
      "9     en_Amsterdam_h1  0  de_Amsterdam_h3  0  1.000000  word2vector\n",
      "10    en_Amsterdam_h1  0  de_Amsterdam_h4  1  1.000000  word2vector\n",
      "11    en_Amsterdam_h1  0  de_Amsterdam_h7  2  1.000000  word2vector\n",
      "12    en_Amsterdam_h1  0  de_Amsterdam_h2  3  1.000000  word2vector\n",
      "13    en_Amsterdam_h1  0  de_Amsterdam_h5  4  1.000000  word2vector\n",
      "14    en_Amsterdam_h1  0  de_Amsterdam_h6  5  0.999999  word2vector\n",
      "15    en_Amsterdam_h1  0  de_Amsterdam_h0  6  0.999999  word2vector\n",
      "16    en_Amsterdam_h1  0  de_Amsterdam_h8  7  0.999998  word2vector\n",
      "17    en_Amsterdam_h1  0  de_Amsterdam_h1  8  0.999998  word2vector\n",
      "18    en_Amsterdam_h2  0  de_Amsterdam_h3  0  1.000000  word2vector\n",
      "19    en_Amsterdam_h2  0  de_Amsterdam_h7  1  1.000000  word2vector\n",
      "20    en_Amsterdam_h2  0  de_Amsterdam_h4  2  1.000000  word2vector\n",
      "21    en_Amsterdam_h2  0  de_Amsterdam_h5  3  1.000000  word2vector\n",
      "22    en_Amsterdam_h2  0  de_Amsterdam_h2  4  1.000000  word2vector\n",
      "23    en_Amsterdam_h2  0  de_Amsterdam_h6  5  1.000000  word2vector\n",
      "24    en_Amsterdam_h2  0  de_Amsterdam_h0  6  1.000000  word2vector\n",
      "25    en_Amsterdam_h2  0  de_Amsterdam_h1  7  0.999998  word2vector\n",
      "26    en_Amsterdam_h2  0  de_Amsterdam_h8  8  0.999998  word2vector\n",
      "27    en_Amsterdam_h3  0  de_Amsterdam_h7  0  1.000000  word2vector\n",
      "28    en_Amsterdam_h3  0  de_Amsterdam_h5  1  1.000000  word2vector\n",
      "29    en_Amsterdam_h3  0  de_Amsterdam_h4  2  1.000000  word2vector\n",
      "..                ... ..              ... ..       ...          ...\n",
      "87    en_Amsterdam_h9  0  de_Amsterdam_h0  6  1.000000  word2vector\n",
      "88    en_Amsterdam_h9  0  de_Amsterdam_h1  7  0.999999  word2vector\n",
      "89    en_Amsterdam_h9  0  de_Amsterdam_h8  8  0.999998  word2vector\n",
      "90   en_Amsterdam_h10  0  de_Amsterdam_h7  0  1.000000  word2vector\n",
      "91   en_Amsterdam_h10  0  de_Amsterdam_h5  1  1.000000  word2vector\n",
      "92   en_Amsterdam_h10  0  de_Amsterdam_h4  2  1.000000  word2vector\n",
      "93   en_Amsterdam_h10  0  de_Amsterdam_h3  3  1.000000  word2vector\n",
      "94   en_Amsterdam_h10  0  de_Amsterdam_h6  4  1.000000  word2vector\n",
      "95   en_Amsterdam_h10  0  de_Amsterdam_h0  5  1.000000  word2vector\n",
      "96   en_Amsterdam_h10  0  de_Amsterdam_h2  6  1.000000  word2vector\n",
      "97   en_Amsterdam_h10  0  de_Amsterdam_h1  7  0.999999  word2vector\n",
      "98   en_Amsterdam_h10  0  de_Amsterdam_h8  8  0.999998  word2vector\n",
      "99   en_Amsterdam_h11  0  de_Amsterdam_h7  0  1.000000  word2vector\n",
      "100  en_Amsterdam_h11  0  de_Amsterdam_h3  1  1.000000  word2vector\n",
      "101  en_Amsterdam_h11  0  de_Amsterdam_h4  2  1.000000  word2vector\n",
      "102  en_Amsterdam_h11  0  de_Amsterdam_h5  3  1.000000  word2vector\n",
      "103  en_Amsterdam_h11  0  de_Amsterdam_h6  4  1.000000  word2vector\n",
      "104  en_Amsterdam_h11  0  de_Amsterdam_h2  5  1.000000  word2vector\n",
      "105  en_Amsterdam_h11  0  de_Amsterdam_h0  6  1.000000  word2vector\n",
      "106  en_Amsterdam_h11  0  de_Amsterdam_h1  7  0.999999  word2vector\n",
      "107  en_Amsterdam_h11  0  de_Amsterdam_h8  8  0.999998  word2vector\n",
      "108  en_Amsterdam_h12  0  de_Amsterdam_h5  0  0.999999  word2vector\n",
      "109  en_Amsterdam_h12  0  de_Amsterdam_h7  1  0.999999  word2vector\n",
      "110  en_Amsterdam_h12  0  de_Amsterdam_h4  2  0.999999  word2vector\n",
      "111  en_Amsterdam_h12  0  de_Amsterdam_h3  3  0.999999  word2vector\n",
      "112  en_Amsterdam_h12  0  de_Amsterdam_h0  4  0.999999  word2vector\n",
      "113  en_Amsterdam_h12  0  de_Amsterdam_h2  5  0.999999  word2vector\n",
      "114  en_Amsterdam_h12  0  de_Amsterdam_h6  6  0.999999  word2vector\n",
      "115  en_Amsterdam_h12  0  de_Amsterdam_h1  7  0.999998  word2vector\n",
      "116  en_Amsterdam_h12  0  de_Amsterdam_h8  8  0.999998  word2vector\n",
      "\n",
      "[117 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Englis-German Aspects Linking Content-Content\n",
    "\n",
    "getWordVectorSpaceModelRun(en_content, de_content, 'en', 'zh', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0  1                2  3         4            5\n",
      "0    en_Amsterdam_h0  0  de_Amsterdam_h7  0  0.999991  word2vector\n",
      "1    en_Amsterdam_h0  0  de_Amsterdam_h0  1  0.999989  word2vector\n",
      "2    en_Amsterdam_h0  0  de_Amsterdam_h1  2  0.999978  word2vector\n",
      "3    en_Amsterdam_h0  0  de_Amsterdam_h6  3  0.999328  word2vector\n",
      "4    en_Amsterdam_h1  0  de_Amsterdam_h7  0  0.999991  word2vector\n",
      "5    en_Amsterdam_h1  0  de_Amsterdam_h0  1  0.999987  word2vector\n",
      "6    en_Amsterdam_h1  0  de_Amsterdam_h1  2  0.999977  word2vector\n",
      "7    en_Amsterdam_h1  0  de_Amsterdam_h6  3  0.999326  word2vector\n",
      "8    en_Amsterdam_h2  0  de_Amsterdam_h7  0  0.999991  word2vector\n",
      "9    en_Amsterdam_h2  0  de_Amsterdam_h0  1  0.999988  word2vector\n",
      "10   en_Amsterdam_h2  0  de_Amsterdam_h1  2  0.999978  word2vector\n",
      "11   en_Amsterdam_h2  0  de_Amsterdam_h6  3  0.999328  word2vector\n",
      "12   en_Amsterdam_h3  0  de_Amsterdam_h7  0  0.999990  word2vector\n",
      "13   en_Amsterdam_h3  0  de_Amsterdam_h0  1  0.999989  word2vector\n",
      "14   en_Amsterdam_h3  0  de_Amsterdam_h1  2  0.999978  word2vector\n",
      "15   en_Amsterdam_h3  0  de_Amsterdam_h6  3  0.999329  word2vector\n",
      "16   en_Amsterdam_h4  0  de_Amsterdam_h7  0  0.999991  word2vector\n",
      "17   en_Amsterdam_h4  0  de_Amsterdam_h0  1  0.999988  word2vector\n",
      "18   en_Amsterdam_h4  0  de_Amsterdam_h1  2  0.999978  word2vector\n",
      "19   en_Amsterdam_h4  0  de_Amsterdam_h6  3  0.999327  word2vector\n",
      "20   en_Amsterdam_h5  0  de_Amsterdam_h7  0  0.999991  word2vector\n",
      "21   en_Amsterdam_h5  0  de_Amsterdam_h0  1  0.999988  word2vector\n",
      "22   en_Amsterdam_h5  0  de_Amsterdam_h1  2  0.999978  word2vector\n",
      "23   en_Amsterdam_h5  0  de_Amsterdam_h6  3  0.999327  word2vector\n",
      "24   en_Amsterdam_h6  0  de_Amsterdam_h7  0  0.999991  word2vector\n",
      "25   en_Amsterdam_h6  0  de_Amsterdam_h0  1  0.999989  word2vector\n",
      "26   en_Amsterdam_h6  0  de_Amsterdam_h1  2  0.999979  word2vector\n",
      "27   en_Amsterdam_h6  0  de_Amsterdam_h6  3  0.999328  word2vector\n",
      "28   en_Amsterdam_h7  0  de_Amsterdam_h7  0  0.999991  word2vector\n",
      "29   en_Amsterdam_h7  0  de_Amsterdam_h0  1  0.999989  word2vector\n",
      "30   en_Amsterdam_h7  0  de_Amsterdam_h1  2  0.999978  word2vector\n",
      "31   en_Amsterdam_h7  0  de_Amsterdam_h6  3  0.999328  word2vector\n",
      "32   en_Amsterdam_h8  0  de_Amsterdam_h7  0  0.999990  word2vector\n",
      "33   en_Amsterdam_h8  0  de_Amsterdam_h0  1  0.999990  word2vector\n",
      "34   en_Amsterdam_h8  0  de_Amsterdam_h1  2  0.999978  word2vector\n",
      "35   en_Amsterdam_h8  0  de_Amsterdam_h6  3  0.999329  word2vector\n",
      "36   en_Amsterdam_h9  0  de_Amsterdam_h7  0  0.999992  word2vector\n",
      "37   en_Amsterdam_h9  0  de_Amsterdam_h0  1  0.999989  word2vector\n",
      "38   en_Amsterdam_h9  0  de_Amsterdam_h1  2  0.999979  word2vector\n",
      "39   en_Amsterdam_h9  0  de_Amsterdam_h6  3  0.999329  word2vector\n",
      "40  en_Amsterdam_h10  0  de_Amsterdam_h7  0  0.999991  word2vector\n",
      "41  en_Amsterdam_h10  0  de_Amsterdam_h0  1  0.999990  word2vector\n",
      "42  en_Amsterdam_h10  0  de_Amsterdam_h1  2  0.999979  word2vector\n",
      "43  en_Amsterdam_h10  0  de_Amsterdam_h6  3  0.999328  word2vector\n",
      "44  en_Amsterdam_h11  0  de_Amsterdam_h7  0  0.999992  word2vector\n",
      "45  en_Amsterdam_h11  0  de_Amsterdam_h0  1  0.999989  word2vector\n",
      "46  en_Amsterdam_h11  0  de_Amsterdam_h1  2  0.999979  word2vector\n",
      "47  en_Amsterdam_h11  0  de_Amsterdam_h6  3  0.999328  word2vector\n",
      "48  en_Amsterdam_h12  0  de_Amsterdam_h7  0  0.999992  word2vector\n",
      "49  en_Amsterdam_h12  0  de_Amsterdam_h0  1  0.999988  word2vector\n",
      "50  en_Amsterdam_h12  0  de_Amsterdam_h1  2  0.999979  word2vector\n",
      "51  en_Amsterdam_h12  0  de_Amsterdam_h6  3  0.999330  word2vector\n"
     ]
    }
   ],
   "source": [
    "# Englis-German Aspects Linking Content-Headline\n",
    "\n",
    "getWordVectorSpaceModelRun(en_content, de_headline, 'en', 'zh', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0  1                2  3         4            5\n",
      "0    en_Amsterdam_h0  0  de_Amsterdam_h1  0  0.999994  word2vector\n",
      "1    en_Amsterdam_h0  0  de_Amsterdam_h8  1  0.999991  word2vector\n",
      "2    en_Amsterdam_h0  0  de_Amsterdam_h0  2  0.999989  word2vector\n",
      "3    en_Amsterdam_h0  0  de_Amsterdam_h7  3  0.999989  word2vector\n",
      "4    en_Amsterdam_h0  0  de_Amsterdam_h6  4  0.999989  word2vector\n",
      "5    en_Amsterdam_h0  0  de_Amsterdam_h5  5  0.999988  word2vector\n",
      "6    en_Amsterdam_h0  0  de_Amsterdam_h4  6  0.999988  word2vector\n",
      "7    en_Amsterdam_h0  0  de_Amsterdam_h3  7  0.999988  word2vector\n",
      "8    en_Amsterdam_h0  0  de_Amsterdam_h2  8  0.999988  word2vector\n",
      "9    en_Amsterdam_h5  0  de_Amsterdam_h3  0  0.999991  word2vector\n",
      "10   en_Amsterdam_h5  0  de_Amsterdam_h7  1  0.999991  word2vector\n",
      "11   en_Amsterdam_h5  0  de_Amsterdam_h5  2  0.999991  word2vector\n",
      "12   en_Amsterdam_h5  0  de_Amsterdam_h6  3  0.999990  word2vector\n",
      "13   en_Amsterdam_h5  0  de_Amsterdam_h4  4  0.999990  word2vector\n",
      "14   en_Amsterdam_h5  0  de_Amsterdam_h0  5  0.999990  word2vector\n",
      "15   en_Amsterdam_h5  0  de_Amsterdam_h2  6  0.999990  word2vector\n",
      "16   en_Amsterdam_h5  0  de_Amsterdam_h8  7  0.999990  word2vector\n",
      "17   en_Amsterdam_h5  0  de_Amsterdam_h1  8  0.999988  word2vector\n",
      "18   en_Amsterdam_h7  0  de_Amsterdam_h8  0  0.999354  word2vector\n",
      "19   en_Amsterdam_h7  0  de_Amsterdam_h1  1  0.999331  word2vector\n",
      "20   en_Amsterdam_h7  0  de_Amsterdam_h7  2  0.999328  word2vector\n",
      "21   en_Amsterdam_h7  0  de_Amsterdam_h6  3  0.999328  word2vector\n",
      "22   en_Amsterdam_h7  0  de_Amsterdam_h2  4  0.999328  word2vector\n",
      "23   en_Amsterdam_h7  0  de_Amsterdam_h4  5  0.999327  word2vector\n",
      "24   en_Amsterdam_h7  0  de_Amsterdam_h0  6  0.999327  word2vector\n",
      "25   en_Amsterdam_h7  0  de_Amsterdam_h5  7  0.999327  word2vector\n",
      "26   en_Amsterdam_h7  0  de_Amsterdam_h3  8  0.999327  word2vector\n",
      "27   en_Amsterdam_h9  0  de_Amsterdam_h1  0  0.999372  word2vector\n",
      "28   en_Amsterdam_h9  0  de_Amsterdam_h5  1  0.999368  word2vector\n",
      "29   en_Amsterdam_h9  0  de_Amsterdam_h0  2  0.999367  word2vector\n",
      "30   en_Amsterdam_h9  0  de_Amsterdam_h7  3  0.999367  word2vector\n",
      "31   en_Amsterdam_h9  0  de_Amsterdam_h2  4  0.999366  word2vector\n",
      "32   en_Amsterdam_h9  0  de_Amsterdam_h4  5  0.999366  word2vector\n",
      "33   en_Amsterdam_h9  0  de_Amsterdam_h3  6  0.999366  word2vector\n",
      "34   en_Amsterdam_h9  0  de_Amsterdam_h6  7  0.999366  word2vector\n",
      "35   en_Amsterdam_h9  0  de_Amsterdam_h8  8  0.999365  word2vector\n",
      "36  en_Amsterdam_h12  0  de_Amsterdam_h5  0  0.999740  word2vector\n",
      "37  en_Amsterdam_h12  0  de_Amsterdam_h4  1  0.999740  word2vector\n",
      "38  en_Amsterdam_h12  0  de_Amsterdam_h2  2  0.999740  word2vector\n",
      "39  en_Amsterdam_h12  0  de_Amsterdam_h3  3  0.999739  word2vector\n",
      "40  en_Amsterdam_h12  0  de_Amsterdam_h7  4  0.999739  word2vector\n",
      "41  en_Amsterdam_h12  0  de_Amsterdam_h1  5  0.999739  word2vector\n",
      "42  en_Amsterdam_h12  0  de_Amsterdam_h8  6  0.999739  word2vector\n",
      "43  en_Amsterdam_h12  0  de_Amsterdam_h0  7  0.999739  word2vector\n",
      "44  en_Amsterdam_h12  0  de_Amsterdam_h6  8  0.999738  word2vector\n"
     ]
    }
   ],
   "source": [
    "# Englis-German Aspects Linking Headline-Content\n",
    "\n",
    "getWordVectorSpaceModelRun(en_headline, de_content, 'en', 'zh', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word Vector Space Model with fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## input: \n",
    "    #1. list of preprocessing documents\n",
    "    #2. language model\n",
    "    #3. entity name\n",
    "    #4. language: En/De\n",
    "    #5. representation of aspects: headline/content\n",
    "## output: for each documnet, a document vector will be produced\n",
    "\n",
    "def getDocVectorsWVSMFinal(content, language_model):\n",
    "    \n",
    "    language_model = language_model\n",
    "\n",
    "    words = []\n",
    "    for word in language_model.vocab:\n",
    "        words.append(word)\n",
    "\n",
    "    doc_vectors = list()\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        #print(content[i])\n",
    "        vectorSum = [0.0000]*300\n",
    "        l = 0\n",
    "        for el in content[i]:\n",
    "            if el in words:\n",
    "                #print(list(vectors[el]))\n",
    "                vectorSum = list(map(add, list(language_model[el]), vectorSum))\n",
    "                l+=1\n",
    "            #else:\n",
    "                #print(el)\n",
    "        #print(vectorSum)\n",
    "        for m in range(len(vectorSum)):\n",
    "            if vectorSum[m] != 0:\n",
    "                vectorSum[m] = float(vectorSum[m])/l ###average the vector sum\n",
    "            else:\n",
    "                vectorSum[m] = vectorSum[m]\n",
    "\n",
    "        doc_vectors.append(vectorSum)\n",
    "    \n",
    "    #doc_vectors_final = []\n",
    "    #for vec in doc_vectors:\n",
    "        #doc = []\n",
    "        #for dim in vec:\n",
    "            \n",
    "            #doc.append(float(dim))\n",
    "        \n",
    "        #doc_vectors_final.append(doc)\n",
    "            \n",
    "        \n",
    "    #with open(language+'_'+termname+'_'+aspect+\".json\", 'w') as f:\n",
    "        #json.dump(doc_vectors_final, f)\n",
    "  \n",
    "    \n",
    "    return doc_vectors \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: 1* n dims sparse matrix, or single vector from query_vect/ train_tfidf matrix\n",
    "def getSquareSumWE(vector):\n",
    "    squaresum = 0\n",
    "    for i in range(len(vector)):\n",
    "        squaresum += vector[i]* vector[i]\n",
    "        \n",
    "    return squaresum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: two 1* n sparse matrixs, or single vector from query_vect/ train_tfidf matrixs\n",
    "# return the cosine sim of two vectors, type: float\n",
    "\n",
    "def getCosineSimilarityWE(query_vector, doc_vector):\n",
    "    squaresum_query = getSquareSumWE(query_vector)\n",
    "    squaresum_doc= getSquareSumWE(doc_vector)\n",
    "    if math.sqrt(squaresum_query)*math.sqrt(squaresum_doc) > 0:\n",
    "        sim = np.dot(query_vector, doc_vector)/(math.sqrt(squaresum_query)*math.sqrt(squaresum_doc))\n",
    "        return sim\n",
    "    else:\n",
    "        return np.dot(query_vector, doc_vector)\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## input:\n",
    "# assume we want to link wiki-text in deutsch(arrow language) with the wiki-text in english(target):\n",
    "# src_content: is the source page data of the wikipedia item in laguage a\n",
    "# trs_content: is the translation data of the wikipedia item from language b to language a\n",
    "# en_model: english language model\n",
    "# de_model: deutsch language\n",
    "# arrow_la: arrow language of the wiki-text\n",
    "# target_la: target language of the wiki-text\n",
    "# termname: entry name of the wiki-text\n",
    "\n",
    "## return: RUN file for Trec_eval\n",
    "\n",
    "def getWordVectorSpace2Run(en_content, de_content, DV_en, DV_de, la1, la2, termname):    \n",
    "    \n",
    "    ### get the similarity of query with each doc leader\n",
    "    \n",
    "    #get document vector for each aspect\n",
    "    DV_en_content = DV_en\n",
    "    DV_de_content = DV_de\n",
    "    \n",
    "    \n",
    "    result = []\n",
    "    for j in range(len(DV_en_content)):\n",
    "        #print (DV_de_content[j][:10])\n",
    "        \n",
    "        sims = []\n",
    "        results_ID = []\n",
    "        results = []\n",
    "        for i in range(len(DV_de_content)):\n",
    "            s = getCosineSimilarityWE(DV_en_content[j], DV_de_content[i])\n",
    "            #if type(s)is not None:\n",
    "            #print(s)\n",
    "            sims.append(s)\n",
    "        \n",
    "        for x in range(len(sims)):\n",
    "            if sims[x] >0:\n",
    "                results_ID.append(x)\n",
    "                results.append(sims[x])\n",
    "        \n",
    "        #print(results, results_ID)\n",
    "        \n",
    "        if len(results) >0 and len(results_ID)>0:\n",
    "            results, results_ID= zip(*sorted(zip(results, results_ID), reverse=True))      \n",
    "       \n",
    "        \n",
    "        for m in range(len(results)):\n",
    "            result.append([la1+'_'+termname+\"_h\"+str(j), 0, la2+'_'+termname+'_h'+str(results_ID[m]), m, results[m], 'wordembedding'])\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    if len(de_content[0]) > 10 and len(en_content[0])>10 :\n",
    "        df.to_csv('WVSMFinal_'+termname+'_'+la1+'_'+la2+'_text.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    elif len(de_content[0]) < 10 and len(en_content[0])<10:\n",
    "        df.to_csv('WVSMFinal_'+termname+'_'+la1+'_'+la2+'_headline.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    elif len(de_content[0]) < 10 and len(en_content[0])>10:\n",
    "        df.to_csv('WVSMFinal_'+termname+'_'+la1+'_'+la2+'_headline_text.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    else:\n",
    "        df.to_csv('WVSMFinal_'+termname+'_'+la1+'_'+la2+'_text_headline.txt', header=None, index=None, sep=' ', mode='a')     \n",
    "    \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DV_en_headline = getDocVectorsWVSMFinal(en_headline, en_model)\n",
    "DV_en_content = getDocVectorsWVSMFinal(en_content, en_model)\n",
    "\n",
    "DV_zh_headline = getDocVectorsWVSMFinal(zh_headline, en_model)\n",
    "DV_zh_content = getDocVectorsWVSMFinal(zh_content, en_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1                2  3         4              5\n",
      "0     en_Amsterdam_h0  0  de_Amsterdam_h0  0  1.000000  wordembedding\n",
      "1     en_Amsterdam_h0  0  de_Amsterdam_h8  1  0.328429  wordembedding\n",
      "2     en_Amsterdam_h0  0  de_Amsterdam_h1  2  0.267095  wordembedding\n",
      "3     en_Amsterdam_h0  0  de_Amsterdam_h6  3  0.263959  wordembedding\n",
      "4     en_Amsterdam_h0  0  de_Amsterdam_h7  4  0.261862  wordembedding\n",
      "5     en_Amsterdam_h0  0  de_Amsterdam_h5  5  0.261492  wordembedding\n",
      "6     en_Amsterdam_h0  0  de_Amsterdam_h2  6  0.242564  wordembedding\n",
      "7     en_Amsterdam_h0  0  de_Amsterdam_h4  7  0.238223  wordembedding\n",
      "8     en_Amsterdam_h0  0  de_Amsterdam_h3  8  0.224148  wordembedding\n",
      "9     en_Amsterdam_h1  0  de_Amsterdam_h2  0  0.812261  wordembedding\n",
      "10    en_Amsterdam_h1  0  de_Amsterdam_h3  1  0.805303  wordembedding\n",
      "11    en_Amsterdam_h1  0  de_Amsterdam_h6  2  0.800517  wordembedding\n",
      "12    en_Amsterdam_h1  0  de_Amsterdam_h4  3  0.786783  wordembedding\n",
      "13    en_Amsterdam_h1  0  de_Amsterdam_h5  4  0.783432  wordembedding\n",
      "14    en_Amsterdam_h1  0  de_Amsterdam_h7  5  0.722408  wordembedding\n",
      "15    en_Amsterdam_h1  0  de_Amsterdam_h8  6  0.685078  wordembedding\n",
      "16    en_Amsterdam_h1  0  de_Amsterdam_h1  7  0.561881  wordembedding\n",
      "17    en_Amsterdam_h1  0  de_Amsterdam_h0  8  0.284131  wordembedding\n",
      "18    en_Amsterdam_h2  0  de_Amsterdam_h3  0  0.975021  wordembedding\n",
      "19    en_Amsterdam_h2  0  de_Amsterdam_h6  1  0.901064  wordembedding\n",
      "20    en_Amsterdam_h2  0  de_Amsterdam_h2  2  0.898076  wordembedding\n",
      "21    en_Amsterdam_h2  0  de_Amsterdam_h4  3  0.886607  wordembedding\n",
      "22    en_Amsterdam_h2  0  de_Amsterdam_h5  4  0.870822  wordembedding\n",
      "23    en_Amsterdam_h2  0  de_Amsterdam_h7  5  0.829649  wordembedding\n",
      "24    en_Amsterdam_h2  0  de_Amsterdam_h8  6  0.739127  wordembedding\n",
      "25    en_Amsterdam_h2  0  de_Amsterdam_h1  7  0.652533  wordembedding\n",
      "26    en_Amsterdam_h2  0  de_Amsterdam_h0  8  0.231957  wordembedding\n",
      "27    en_Amsterdam_h3  0  de_Amsterdam_h2  0  0.964203  wordembedding\n",
      "28    en_Amsterdam_h3  0  de_Amsterdam_h4  1  0.839497  wordembedding\n",
      "29    en_Amsterdam_h3  0  de_Amsterdam_h3  2  0.838500  wordembedding\n",
      "..                ... ..              ... ..       ...            ...\n",
      "87    en_Amsterdam_h9  0  de_Amsterdam_h8  6  0.679933  wordembedding\n",
      "88    en_Amsterdam_h9  0  de_Amsterdam_h1  7  0.643496  wordembedding\n",
      "89    en_Amsterdam_h9  0  de_Amsterdam_h0  8  0.270819  wordembedding\n",
      "90   en_Amsterdam_h10  0  de_Amsterdam_h4  0  0.865455  wordembedding\n",
      "91   en_Amsterdam_h10  0  de_Amsterdam_h6  1  0.865411  wordembedding\n",
      "92   en_Amsterdam_h10  0  de_Amsterdam_h7  2  0.857761  wordembedding\n",
      "93   en_Amsterdam_h10  0  de_Amsterdam_h2  3  0.855960  wordembedding\n",
      "94   en_Amsterdam_h10  0  de_Amsterdam_h3  4  0.844966  wordembedding\n",
      "95   en_Amsterdam_h10  0  de_Amsterdam_h5  5  0.839779  wordembedding\n",
      "96   en_Amsterdam_h10  0  de_Amsterdam_h1  6  0.711583  wordembedding\n",
      "97   en_Amsterdam_h10  0  de_Amsterdam_h8  7  0.704832  wordembedding\n",
      "98   en_Amsterdam_h10  0  de_Amsterdam_h0  8  0.246337  wordembedding\n",
      "99   en_Amsterdam_h11  0  de_Amsterdam_h6  0  0.850093  wordembedding\n",
      "100  en_Amsterdam_h11  0  de_Amsterdam_h4  1  0.843746  wordembedding\n",
      "101  en_Amsterdam_h11  0  de_Amsterdam_h3  2  0.821361  wordembedding\n",
      "102  en_Amsterdam_h11  0  de_Amsterdam_h5  3  0.800633  wordembedding\n",
      "103  en_Amsterdam_h11  0  de_Amsterdam_h7  4  0.790284  wordembedding\n",
      "104  en_Amsterdam_h11  0  de_Amsterdam_h2  5  0.790162  wordembedding\n",
      "105  en_Amsterdam_h11  0  de_Amsterdam_h8  6  0.715778  wordembedding\n",
      "106  en_Amsterdam_h11  0  de_Amsterdam_h1  7  0.626506  wordembedding\n",
      "107  en_Amsterdam_h11  0  de_Amsterdam_h0  8  0.273100  wordembedding\n",
      "108  en_Amsterdam_h12  0  de_Amsterdam_h7  0  0.809991  wordembedding\n",
      "109  en_Amsterdam_h12  0  de_Amsterdam_h5  1  0.790265  wordembedding\n",
      "110  en_Amsterdam_h12  0  de_Amsterdam_h6  2  0.784625  wordembedding\n",
      "111  en_Amsterdam_h12  0  de_Amsterdam_h4  3  0.771168  wordembedding\n",
      "112  en_Amsterdam_h12  0  de_Amsterdam_h3  4  0.765635  wordembedding\n",
      "113  en_Amsterdam_h12  0  de_Amsterdam_h2  5  0.759581  wordembedding\n",
      "114  en_Amsterdam_h12  0  de_Amsterdam_h1  6  0.690742  wordembedding\n",
      "115  en_Amsterdam_h12  0  de_Amsterdam_h8  7  0.686300  wordembedding\n",
      "116  en_Amsterdam_h12  0  de_Amsterdam_h0  8  0.262313  wordembedding\n",
      "\n",
      "[117 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "getWordVectorSpace2Run(en_headline, zh_headline, DV_en_headline, DV_zh_headline, 'en','zh', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1                2  3         4              5\n",
      "0     en_Amsterdam_h0  0  de_Amsterdam_h1  0  0.600057  wordembedding\n",
      "1     en_Amsterdam_h0  0  de_Amsterdam_h8  1  0.536614  wordembedding\n",
      "2     en_Amsterdam_h0  0  de_Amsterdam_h0  2  0.457167  wordembedding\n",
      "3     en_Amsterdam_h0  0  de_Amsterdam_h7  3  0.402822  wordembedding\n",
      "4     en_Amsterdam_h0  0  de_Amsterdam_h6  4  0.386379  wordembedding\n",
      "5     en_Amsterdam_h0  0  de_Amsterdam_h3  5  0.326772  wordembedding\n",
      "6     en_Amsterdam_h0  0  de_Amsterdam_h4  6  0.310785  wordembedding\n",
      "7     en_Amsterdam_h0  0  de_Amsterdam_h2  7  0.302455  wordembedding\n",
      "8     en_Amsterdam_h0  0  de_Amsterdam_h5  8  0.299950  wordembedding\n",
      "9     en_Amsterdam_h1  0  de_Amsterdam_h4  0  0.498023  wordembedding\n",
      "10    en_Amsterdam_h1  0  de_Amsterdam_h6  1  0.494421  wordembedding\n",
      "11    en_Amsterdam_h1  0  de_Amsterdam_h3  2  0.492393  wordembedding\n",
      "12    en_Amsterdam_h1  0  de_Amsterdam_h0  3  0.487803  wordembedding\n",
      "13    en_Amsterdam_h1  0  de_Amsterdam_h7  4  0.487678  wordembedding\n",
      "14    en_Amsterdam_h1  0  de_Amsterdam_h5  5  0.483735  wordembedding\n",
      "15    en_Amsterdam_h1  0  de_Amsterdam_h1  6  0.480922  wordembedding\n",
      "16    en_Amsterdam_h1  0  de_Amsterdam_h2  7  0.457532  wordembedding\n",
      "17    en_Amsterdam_h1  0  de_Amsterdam_h8  8  0.449589  wordembedding\n",
      "18    en_Amsterdam_h2  0  de_Amsterdam_h6  0  0.590217  wordembedding\n",
      "19    en_Amsterdam_h2  0  de_Amsterdam_h4  1  0.584768  wordembedding\n",
      "20    en_Amsterdam_h2  0  de_Amsterdam_h3  2  0.582073  wordembedding\n",
      "21    en_Amsterdam_h2  0  de_Amsterdam_h7  3  0.572952  wordembedding\n",
      "22    en_Amsterdam_h2  0  de_Amsterdam_h0  4  0.564481  wordembedding\n",
      "23    en_Amsterdam_h2  0  de_Amsterdam_h5  5  0.563617  wordembedding\n",
      "24    en_Amsterdam_h2  0  de_Amsterdam_h2  6  0.553149  wordembedding\n",
      "25    en_Amsterdam_h2  0  de_Amsterdam_h1  7  0.534876  wordembedding\n",
      "26    en_Amsterdam_h2  0  de_Amsterdam_h8  8  0.513638  wordembedding\n",
      "27    en_Amsterdam_h3  0  de_Amsterdam_h7  0  0.569587  wordembedding\n",
      "28    en_Amsterdam_h3  0  de_Amsterdam_h3  1  0.568207  wordembedding\n",
      "29    en_Amsterdam_h3  0  de_Amsterdam_h0  2  0.566514  wordembedding\n",
      "..                ... ..              ... ..       ...            ...\n",
      "87    en_Amsterdam_h9  0  de_Amsterdam_h2  6  0.560215  wordembedding\n",
      "88    en_Amsterdam_h9  0  de_Amsterdam_h1  7  0.541845  wordembedding\n",
      "89    en_Amsterdam_h9  0  de_Amsterdam_h8  8  0.458161  wordembedding\n",
      "90   en_Amsterdam_h10  0  de_Amsterdam_h6  0  0.591522  wordembedding\n",
      "91   en_Amsterdam_h10  0  de_Amsterdam_h3  1  0.590007  wordembedding\n",
      "92   en_Amsterdam_h10  0  de_Amsterdam_h4  2  0.589611  wordembedding\n",
      "93   en_Amsterdam_h10  0  de_Amsterdam_h7  3  0.583166  wordembedding\n",
      "94   en_Amsterdam_h10  0  de_Amsterdam_h5  4  0.570999  wordembedding\n",
      "95   en_Amsterdam_h10  0  de_Amsterdam_h0  5  0.566789  wordembedding\n",
      "96   en_Amsterdam_h10  0  de_Amsterdam_h2  6  0.556008  wordembedding\n",
      "97   en_Amsterdam_h10  0  de_Amsterdam_h1  7  0.544053  wordembedding\n",
      "98   en_Amsterdam_h10  0  de_Amsterdam_h8  8  0.486079  wordembedding\n",
      "99   en_Amsterdam_h11  0  de_Amsterdam_h6  0  0.576459  wordembedding\n",
      "100  en_Amsterdam_h11  0  de_Amsterdam_h4  1  0.567251  wordembedding\n",
      "101  en_Amsterdam_h11  0  de_Amsterdam_h7  2  0.561341  wordembedding\n",
      "102  en_Amsterdam_h11  0  de_Amsterdam_h3  3  0.556744  wordembedding\n",
      "103  en_Amsterdam_h11  0  de_Amsterdam_h5  4  0.544571  wordembedding\n",
      "104  en_Amsterdam_h11  0  de_Amsterdam_h0  5  0.544562  wordembedding\n",
      "105  en_Amsterdam_h11  0  de_Amsterdam_h2  6  0.530030  wordembedding\n",
      "106  en_Amsterdam_h11  0  de_Amsterdam_h1  7  0.526004  wordembedding\n",
      "107  en_Amsterdam_h11  0  de_Amsterdam_h8  8  0.501045  wordembedding\n",
      "108  en_Amsterdam_h12  0  de_Amsterdam_h7  0  0.578862  wordembedding\n",
      "109  en_Amsterdam_h12  0  de_Amsterdam_h5  1  0.578573  wordembedding\n",
      "110  en_Amsterdam_h12  0  de_Amsterdam_h4  2  0.576760  wordembedding\n",
      "111  en_Amsterdam_h12  0  de_Amsterdam_h3  3  0.576708  wordembedding\n",
      "112  en_Amsterdam_h12  0  de_Amsterdam_h6  4  0.575212  wordembedding\n",
      "113  en_Amsterdam_h12  0  de_Amsterdam_h0  5  0.561606  wordembedding\n",
      "114  en_Amsterdam_h12  0  de_Amsterdam_h2  6  0.548804  wordembedding\n",
      "115  en_Amsterdam_h12  0  de_Amsterdam_h1  7  0.540359  wordembedding\n",
      "116  en_Amsterdam_h12  0  de_Amsterdam_h8  8  0.468857  wordembedding\n",
      "\n",
      "[117 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "getWordVectorSpace2Run(en_headline, zh_content, DV_en_headline, DV_zh_content, 'en','zv', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1                2  3         4              5\n",
      "0     en_Amsterdam_h0  0  de_Amsterdam_h7  0  0.989807  wordembedding\n",
      "1     en_Amsterdam_h0  0  de_Amsterdam_h0  1  0.984952  wordembedding\n",
      "2     en_Amsterdam_h0  0  de_Amsterdam_h3  2  0.984568  wordembedding\n",
      "3     en_Amsterdam_h0  0  de_Amsterdam_h6  3  0.983110  wordembedding\n",
      "4     en_Amsterdam_h0  0  de_Amsterdam_h4  4  0.981431  wordembedding\n",
      "5     en_Amsterdam_h0  0  de_Amsterdam_h2  5  0.969671  wordembedding\n",
      "6     en_Amsterdam_h0  0  de_Amsterdam_h5  6  0.969355  wordembedding\n",
      "7     en_Amsterdam_h0  0  de_Amsterdam_h1  7  0.940698  wordembedding\n",
      "8     en_Amsterdam_h0  0  de_Amsterdam_h8  8  0.766341  wordembedding\n",
      "9     en_Amsterdam_h1  0  de_Amsterdam_h3  0  0.975771  wordembedding\n",
      "10    en_Amsterdam_h1  0  de_Amsterdam_h4  1  0.969266  wordembedding\n",
      "11    en_Amsterdam_h1  0  de_Amsterdam_h5  2  0.963544  wordembedding\n",
      "12    en_Amsterdam_h1  0  de_Amsterdam_h7  3  0.961434  wordembedding\n",
      "13    en_Amsterdam_h1  0  de_Amsterdam_h2  4  0.959964  wordembedding\n",
      "14    en_Amsterdam_h1  0  de_Amsterdam_h0  5  0.953676  wordembedding\n",
      "15    en_Amsterdam_h1  0  de_Amsterdam_h6  6  0.947847  wordembedding\n",
      "16    en_Amsterdam_h1  0  de_Amsterdam_h1  7  0.891015  wordembedding\n",
      "17    en_Amsterdam_h1  0  de_Amsterdam_h8  8  0.717106  wordembedding\n",
      "18    en_Amsterdam_h2  0  de_Amsterdam_h3  0  0.995898  wordembedding\n",
      "19    en_Amsterdam_h2  0  de_Amsterdam_h4  1  0.990135  wordembedding\n",
      "20    en_Amsterdam_h2  0  de_Amsterdam_h7  2  0.986941  wordembedding\n",
      "21    en_Amsterdam_h2  0  de_Amsterdam_h6  3  0.982826  wordembedding\n",
      "22    en_Amsterdam_h2  0  de_Amsterdam_h5  4  0.982336  wordembedding\n",
      "23    en_Amsterdam_h2  0  de_Amsterdam_h2  5  0.975447  wordembedding\n",
      "24    en_Amsterdam_h2  0  de_Amsterdam_h0  6  0.972158  wordembedding\n",
      "25    en_Amsterdam_h2  0  de_Amsterdam_h1  7  0.921765  wordembedding\n",
      "26    en_Amsterdam_h2  0  de_Amsterdam_h8  8  0.733397  wordembedding\n",
      "27    en_Amsterdam_h3  0  de_Amsterdam_h2  0  0.982523  wordembedding\n",
      "28    en_Amsterdam_h3  0  de_Amsterdam_h5  1  0.973876  wordembedding\n",
      "29    en_Amsterdam_h3  0  de_Amsterdam_h7  2  0.971229  wordembedding\n",
      "..                ... ..              ... ..       ...            ...\n",
      "87    en_Amsterdam_h9  0  de_Amsterdam_h2  6  0.961589  wordembedding\n",
      "88    en_Amsterdam_h9  0  de_Amsterdam_h1  7  0.940746  wordembedding\n",
      "89    en_Amsterdam_h9  0  de_Amsterdam_h8  8  0.764501  wordembedding\n",
      "90   en_Amsterdam_h10  0  de_Amsterdam_h6  0  0.975692  wordembedding\n",
      "91   en_Amsterdam_h10  0  de_Amsterdam_h7  1  0.974942  wordembedding\n",
      "92   en_Amsterdam_h10  0  de_Amsterdam_h3  2  0.970035  wordembedding\n",
      "93   en_Amsterdam_h10  0  de_Amsterdam_h4  3  0.967115  wordembedding\n",
      "94   en_Amsterdam_h10  0  de_Amsterdam_h0  4  0.962993  wordembedding\n",
      "95   en_Amsterdam_h10  0  de_Amsterdam_h5  5  0.958567  wordembedding\n",
      "96   en_Amsterdam_h10  0  de_Amsterdam_h2  6  0.946628  wordembedding\n",
      "97   en_Amsterdam_h10  0  de_Amsterdam_h1  7  0.937869  wordembedding\n",
      "98   en_Amsterdam_h10  0  de_Amsterdam_h8  8  0.752681  wordembedding\n",
      "99   en_Amsterdam_h11  0  de_Amsterdam_h7  0  0.979671  wordembedding\n",
      "100  en_Amsterdam_h11  0  de_Amsterdam_h6  1  0.977101  wordembedding\n",
      "101  en_Amsterdam_h11  0  de_Amsterdam_h4  2  0.973265  wordembedding\n",
      "102  en_Amsterdam_h11  0  de_Amsterdam_h3  3  0.972391  wordembedding\n",
      "103  en_Amsterdam_h11  0  de_Amsterdam_h0  4  0.965455  wordembedding\n",
      "104  en_Amsterdam_h11  0  de_Amsterdam_h5  5  0.964500  wordembedding\n",
      "105  en_Amsterdam_h11  0  de_Amsterdam_h2  6  0.957269  wordembedding\n",
      "106  en_Amsterdam_h11  0  de_Amsterdam_h1  7  0.926663  wordembedding\n",
      "107  en_Amsterdam_h11  0  de_Amsterdam_h8  8  0.756259  wordembedding\n",
      "108  en_Amsterdam_h12  0  de_Amsterdam_h5  0  0.962317  wordembedding\n",
      "109  en_Amsterdam_h12  0  de_Amsterdam_h4  1  0.957449  wordembedding\n",
      "110  en_Amsterdam_h12  0  de_Amsterdam_h3  2  0.956840  wordembedding\n",
      "111  en_Amsterdam_h12  0  de_Amsterdam_h7  3  0.953854  wordembedding\n",
      "112  en_Amsterdam_h12  0  de_Amsterdam_h6  4  0.949990  wordembedding\n",
      "113  en_Amsterdam_h12  0  de_Amsterdam_h2  5  0.935500  wordembedding\n",
      "114  en_Amsterdam_h12  0  de_Amsterdam_h0  6  0.929411  wordembedding\n",
      "115  en_Amsterdam_h12  0  de_Amsterdam_h1  7  0.892477  wordembedding\n",
      "116  en_Amsterdam_h12  0  de_Amsterdam_h8  8  0.695330  wordembedding\n",
      "\n",
      "[117 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "getWordVectorSpace2Run(en_content, zh_content, DV_en_content, DV_zh_content, 'en','zh', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0  1                2  3         4              5\n",
      "0     en_Amsterdam_h0  0  de_Amsterdam_h1  0  0.761034  wordembedding\n",
      "1     en_Amsterdam_h0  0  de_Amsterdam_h7  1  0.702296  wordembedding\n",
      "2     en_Amsterdam_h0  0  de_Amsterdam_h8  2  0.549765  wordembedding\n",
      "3     en_Amsterdam_h0  0  de_Amsterdam_h6  3  0.526005  wordembedding\n",
      "4     en_Amsterdam_h0  0  de_Amsterdam_h5  4  0.519510  wordembedding\n",
      "5     en_Amsterdam_h0  0  de_Amsterdam_h2  5  0.516462  wordembedding\n",
      "6     en_Amsterdam_h0  0  de_Amsterdam_h3  6  0.503821  wordembedding\n",
      "7     en_Amsterdam_h0  0  de_Amsterdam_h4  7  0.497746  wordembedding\n",
      "8     en_Amsterdam_h0  0  de_Amsterdam_h0  8  0.386591  wordembedding\n",
      "9     en_Amsterdam_h1  0  de_Amsterdam_h1  0  0.743821  wordembedding\n",
      "10    en_Amsterdam_h1  0  de_Amsterdam_h7  1  0.670341  wordembedding\n",
      "11    en_Amsterdam_h1  0  de_Amsterdam_h8  2  0.529771  wordembedding\n",
      "12    en_Amsterdam_h1  0  de_Amsterdam_h2  3  0.496139  wordembedding\n",
      "13    en_Amsterdam_h1  0  de_Amsterdam_h5  4  0.493726  wordembedding\n",
      "14    en_Amsterdam_h1  0  de_Amsterdam_h6  5  0.492961  wordembedding\n",
      "15    en_Amsterdam_h1  0  de_Amsterdam_h3  6  0.482027  wordembedding\n",
      "16    en_Amsterdam_h1  0  de_Amsterdam_h4  7  0.473275  wordembedding\n",
      "17    en_Amsterdam_h1  0  de_Amsterdam_h0  8  0.288607  wordembedding\n",
      "18    en_Amsterdam_h2  0  de_Amsterdam_h1  0  0.755112  wordembedding\n",
      "19    en_Amsterdam_h2  0  de_Amsterdam_h7  1  0.702943  wordembedding\n",
      "20    en_Amsterdam_h2  0  de_Amsterdam_h8  2  0.543427  wordembedding\n",
      "21    en_Amsterdam_h2  0  de_Amsterdam_h6  3  0.520682  wordembedding\n",
      "22    en_Amsterdam_h2  0  de_Amsterdam_h5  4  0.517214  wordembedding\n",
      "23    en_Amsterdam_h2  0  de_Amsterdam_h2  5  0.511469  wordembedding\n",
      "24    en_Amsterdam_h2  0  de_Amsterdam_h3  6  0.503733  wordembedding\n",
      "25    en_Amsterdam_h2  0  de_Amsterdam_h4  7  0.501514  wordembedding\n",
      "26    en_Amsterdam_h2  0  de_Amsterdam_h0  8  0.337022  wordembedding\n",
      "27    en_Amsterdam_h3  0  de_Amsterdam_h1  0  0.743718  wordembedding\n",
      "28    en_Amsterdam_h3  0  de_Amsterdam_h7  1  0.670156  wordembedding\n",
      "29    en_Amsterdam_h3  0  de_Amsterdam_h8  2  0.551619  wordembedding\n",
      "..                ... ..              ... ..       ...            ...\n",
      "87    en_Amsterdam_h9  0  de_Amsterdam_h4  6  0.497287  wordembedding\n",
      "88    en_Amsterdam_h9  0  de_Amsterdam_h3  7  0.492171  wordembedding\n",
      "89    en_Amsterdam_h9  0  de_Amsterdam_h0  8  0.410578  wordembedding\n",
      "90   en_Amsterdam_h10  0  de_Amsterdam_h1  0  0.762313  wordembedding\n",
      "91   en_Amsterdam_h10  0  de_Amsterdam_h7  1  0.694069  wordembedding\n",
      "92   en_Amsterdam_h10  0  de_Amsterdam_h8  2  0.554721  wordembedding\n",
      "93   en_Amsterdam_h10  0  de_Amsterdam_h6  3  0.537657  wordembedding\n",
      "94   en_Amsterdam_h10  0  de_Amsterdam_h5  4  0.532465  wordembedding\n",
      "95   en_Amsterdam_h10  0  de_Amsterdam_h2  5  0.528884  wordembedding\n",
      "96   en_Amsterdam_h10  0  de_Amsterdam_h4  6  0.512698  wordembedding\n",
      "97   en_Amsterdam_h10  0  de_Amsterdam_h3  7  0.511519  wordembedding\n",
      "98   en_Amsterdam_h10  0  de_Amsterdam_h0  8  0.429295  wordembedding\n",
      "99   en_Amsterdam_h11  0  de_Amsterdam_h1  0  0.745804  wordembedding\n",
      "100  en_Amsterdam_h11  0  de_Amsterdam_h7  1  0.693314  wordembedding\n",
      "101  en_Amsterdam_h11  0  de_Amsterdam_h8  2  0.560521  wordembedding\n",
      "102  en_Amsterdam_h11  0  de_Amsterdam_h6  3  0.531718  wordembedding\n",
      "103  en_Amsterdam_h11  0  de_Amsterdam_h5  4  0.522442  wordembedding\n",
      "104  en_Amsterdam_h11  0  de_Amsterdam_h2  5  0.516155  wordembedding\n",
      "105  en_Amsterdam_h11  0  de_Amsterdam_h4  6  0.510714  wordembedding\n",
      "106  en_Amsterdam_h11  0  de_Amsterdam_h3  7  0.506733  wordembedding\n",
      "107  en_Amsterdam_h11  0  de_Amsterdam_h0  8  0.399794  wordembedding\n",
      "108  en_Amsterdam_h12  0  de_Amsterdam_h1  0  0.757653  wordembedding\n",
      "109  en_Amsterdam_h12  0  de_Amsterdam_h7  1  0.684343  wordembedding\n",
      "110  en_Amsterdam_h12  0  de_Amsterdam_h8  2  0.503199  wordembedding\n",
      "111  en_Amsterdam_h12  0  de_Amsterdam_h5  3  0.492890  wordembedding\n",
      "112  en_Amsterdam_h12  0  de_Amsterdam_h6  4  0.486041  wordembedding\n",
      "113  en_Amsterdam_h12  0  de_Amsterdam_h2  5  0.474798  wordembedding\n",
      "114  en_Amsterdam_h12  0  de_Amsterdam_h4  6  0.467368  wordembedding\n",
      "115  en_Amsterdam_h12  0  de_Amsterdam_h3  7  0.459507  wordembedding\n",
      "116  en_Amsterdam_h12  0  de_Amsterdam_h0  8  0.304977  wordembedding\n",
      "\n",
      "[117 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "getWordVectorSpace2Run(en_content, zh_headline, DV_en_content, DV_zh_headline, 'en','zh', entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
