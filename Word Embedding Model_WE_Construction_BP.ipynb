{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from operator import add\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from string import punctuation\n",
    "import jieba\n",
    "import hanziconv\n",
    "from hanziconv import HanziConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditionalToSimplified(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        file[i] = ''.join(HanziConv.toSimplified(el))\n",
    "        i += 1\n",
    "        \n",
    "jieba_stop_words = [\n",
    "    '的', '了', '和', '是', '就', '都', '而', '及', '與', \n",
    "    '著', '或', '一個', '沒有', '我們', '你們', '妳們', \n",
    "    '他們', '她們', '是否']       \n",
    "\n",
    "def chineseTokenize(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        file[i] = ' '.join(jieba.cut(el, cut_all=False, HMM=True))\n",
    "        i += 1\n",
    "def chineseStopwordsRemoval(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        file[i] = ' '.join(word for word in el if word not in jieba_stop_words)\n",
    "\n",
    "\n",
    "punctuation = punctuation + str('；')+  str(\"：《》「 」“”[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\")+str('编辑')+str('%.[')+str('(/')+str(');[')+str('\"),')+str(').')+str('.[')+str(',[')+str('][')+str('(\"')+str('.\"[')+str('—'+str('.\"'))+str('.,')\n",
    "def removePunctuation(file):\n",
    "    \n",
    "    i = 0\n",
    "    for el in file:\n",
    "        el = ' '.join([word for word in el.lower().split() if word not in punctuation])\n",
    "        file[i] = el\n",
    "        i += 1\n",
    "\n",
    "\n",
    "from nltk.tokenize import MWETokenizer\n",
    "def MWEtokenize(el):\n",
    "    i = 0\n",
    "    tokenizer = MWETokenizer(('barack','obama'))\n",
    "    tokenizer.add_mwe([('new','york'),('hong', 'kong'), ('los', 'angeles'), ('san', 'francisco'),('united', 'kingdom')])\n",
    "    el = tokenizer.tokenize(el.split())\n",
    "    return el\n",
    "        \n",
    "def tokenize(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        tokenizerOne = WordPunctTokenizer()\n",
    "        el = tokenizerOne.tokenize(str(el.lower()))\n",
    "        el = ' '.join([word for word in el if word not in punctuation])\n",
    "        file[i] = MWEtokenize(el)\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "punctuation = punctuation + str('；')+  str(\"：《》「 」“”[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\")+str('编辑')+str('%.[')+str('(/')+str(');[')+str('\"),')+str(').')+str('.[')+str(',[')+str('][')+str('(\"')+str('.\"[')+str('—'+str('.\"'))+str('.,')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document-vector building function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input: \n",
    "    #1. list of preprocessing documents\n",
    "    #2. language model\n",
    "    #3. entity name\n",
    "    #4. language: En/De\n",
    "    #5. representation of aspects: headline/content\n",
    "## output: for each documnet, a document vector will be produced\n",
    "\n",
    "def getDocVectors(content, language_model, termname ,language, aspect, WE_source):\n",
    "    \n",
    "    language_model = language_model\n",
    "\n",
    "    words = []\n",
    "    for word in language_model.vocab:\n",
    "        words.append(word)\n",
    "\n",
    "    doc_vectors = list()\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        #print(content[i])\n",
    "        vectorSum = [0.0000]*300\n",
    "        l = 0\n",
    "        for el in content[i]:\n",
    "            if el in words:\n",
    "                #print(list(vectors[el]))\n",
    "                vectorSum = list(map(add, list(language_model[el]), vectorSum))\n",
    "                l+=1\n",
    "            #else:\n",
    "                #print(el)\n",
    "        #print(vectorSum)\n",
    "        for m in range(len(vectorSum)):\n",
    "            if vectorSum[m] != 0:\n",
    "                vectorSum[m] = float(vectorSum[m])/l ###average the vector sum\n",
    "            else:\n",
    "                vectorSum[m] = vectorSum[m]\n",
    "\n",
    "        doc_vectors.append(vectorSum)\n",
    "    \n",
    "    doc_vectors_final = []\n",
    "    for vec in doc_vectors:\n",
    "        doc = []\n",
    "        for dim in vec:\n",
    "            \n",
    "            doc.append(float(dim))\n",
    "        \n",
    "        doc_vectors_final.append(doc)\n",
    "            \n",
    "        \n",
    "    with open(WE_source+'_'+language+'_'+termname+'_'+aspect+\".json\", 'w') as f:\n",
    "        json.dump(doc_vectors_final, f)\n",
    "  \n",
    "    \n",
    "    #return doc_vectors_final \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the function! get and save the document vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Ebedding _BP_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading English word embeddings...\n",
      "loading Chinese word embeddings...\n",
      "Number of English Tokens: 2519370\n",
      "Number of Chinese Tokens: 332647\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# importing wordembedding and building the language model\n",
    "\n",
    "print('loading English word embeddings...')\n",
    "en_model = KeyedVectors.load_word2vec_format('/home/hahou/WordEmbedding/wiki.en.transformed.vec')\n",
    "\n",
    "'''print('loading German word embeddings...')\n",
    "de_model = KeyedVectors.load_word2vec_format('/home/hahou/WordEmbedding/wiki.de.transformed.vec')'''\n",
    "\n",
    "print('loading Chinese word embeddings...')\n",
    "zh_model = KeyedVectors.load_word2vec_format('/home/hahou/WordEmbedding/wiki.zh.transformed.vec')\n",
    "\n",
    "# Getting the tokens \n",
    "en_words = []\n",
    "for word in en_model.vocab:\n",
    "    en_words.append(word)\n",
    "\n",
    "# Printing out number of tokens available\n",
    "print(\"Number of English Tokens: {}\".format(len(en_words)))\n",
    "\n",
    "'''de_words = []\n",
    "for word in de_model.vocab:\n",
    "    de_words.append(word)\n",
    "\n",
    "# Printing out number of tokens available\n",
    "print(\"Number of German Tokens: {}\".format(len(de_words)))'''\n",
    "\n",
    "zh_words = []\n",
    "for word in zh_model.vocab:\n",
    "    zh_words.append(word)\n",
    "\n",
    "# Printing out number of tokens available\n",
    "print(\"Number of Chinese Tokens: {}\".format(len(zh_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: United_States\n"
     ]
    }
   ],
   "source": [
    "# define the entity list\n",
    "#entity_list = ['United_Kingdom', 'Italy', 'Asia','Europe','Canada', 'China', 'France', 'Germany', 'Japan']\n",
    "\n",
    "#entity_list =  ['Russia','singapore','India', 'Israel','Brazil','Philippines'] \n",
    "#entity_list = ['Barack_Obama', 'Donald_Trump','New_York_City',\n",
    "#'London','Singapore','Hong_Kong','Dubai','Los_Angeles','Paris','Chicago','Washington,_D.C.','San_Francisco',\n",
    "#'Mumbai','Rome','Toronto','Philadelphia','Monaco','Tokyo','Amsterdam','Boston','Barcelona','Peking']\n",
    "\n",
    "#entity_list = ['United_States']\n",
    "\n",
    "vector_source = 'BP'\n",
    "\n",
    "for entity in entity_list:\n",
    "    # loading source data\n",
    "    print('entity: '+entity)\n",
    "    with open('/home/hahou/WikiDataCrawling/English Corpus/source_en_'+entity+'.json') as json_data:\n",
    "        source_en = json.load(json_data)\n",
    "    '''with open('/home/hahou/WikiDataCrawling/German Corpus/source_de_'+entity+'.json') as json_data:\n",
    "        source_de = json.load(json_data)'''\n",
    "    with open('/home/hahou/WikiDataCrawling/Chinese Corpus/source_zh_'+entity+'.json') as json_data:\n",
    "        source_zh = json.load(json_data)    \n",
    "    \n",
    "    # exact headlies\n",
    "    en_headline = list(source_en.keys())\n",
    "    #de_headline = list(source_de.keys())\n",
    "    zh_headline = list(source_zh.keys())\n",
    "\n",
    "    #extract context\n",
    "    en_content = []\n",
    "    #de_content = []\n",
    "    zh_content = []\n",
    "    for el in en_headline:\n",
    "        en_content.append(''.join(source_en[el]))\n",
    "    #for el in de_headline:\n",
    "     #   de_content.append(''.join(source_de[el]))\n",
    "    for el in zh_headline:\n",
    "        zh_content.append(''.join(source_zh[el]))\n",
    "        \n",
    "    # preprocessing for en and de\n",
    "    tokenize(en_content)\n",
    "    #tokenize(de_content)\n",
    "    tokenize(en_headline)\n",
    "    #tokenize(de_headline)\n",
    "    \n",
    "    \n",
    "    #preprocessing for zh\n",
    "    traditionalToSimplified(zh_content)\n",
    "    chineseTokenize(zh_content)\n",
    "    removePunctuation(zh_content)\n",
    "    chineseStopwordsRemoval(zh_content)\n",
    "\n",
    "    traditionalToSimplified(zh_headline)\n",
    "    chineseTokenize(zh_headline)\n",
    "    removePunctuation(zh_headline)\n",
    "    chineseStopwordsRemoval(zh_headline)\n",
    "    \n",
    "    \n",
    "    # get the document vectors for headlines of entry page section\n",
    "    getDocVectors(en_headline, en_model, entity,'En', 'Headline', vector_source)\n",
    "    #getDocVectors(de_headline, de_model, entity,'De', 'Headline', vector_source)\n",
    "    getDocVectors(zh_headline, zh_model, entity,'zh', 'Headline', vector_source)\n",
    "    \n",
    "    get the document vectors for Contents of entry page section\n",
    "    #getDocVectors(en_content, en_model, entity,'En', 'Content', vector_source)\n",
    "    #getDocVectors(de_content, de_model, entity,'De', 'Content', vector_source)\n",
    "    getDocVectors(zh_content, zh_model, entity,'zh', 'Content', vector_source)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
