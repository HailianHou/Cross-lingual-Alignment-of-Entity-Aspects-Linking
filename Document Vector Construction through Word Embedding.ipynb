{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from operator import add\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# importing wordembedding and building the language model\n",
    "de_model = KeyedVectors.load_word2vec_format('/home/hahou/WordEmbedding/wiki.multi.en.vec')\n",
    "en_model = KeyedVectors.load_word2vec_format('/home/hahou/WordEmbedding/wiki.multi.de.vec')\n",
    "zh_model = KeyedVectors.load_word2vec_format('/home/hahou/WordEmbedding/wiki.zh.align.vec')\n",
    "\n",
    "# Getting the tokens \n",
    "en_words = []\n",
    "for word in en_model.vocab:\n",
    "    en_words.append(word)\n",
    "\n",
    "# Printing out number of tokens available\n",
    "print(\"Number of English Tokens: {}\".format(len(en_words)))\n",
    "\n",
    "de_words = []\n",
    "for word in de_model.vocab:\n",
    "    de_words.append(word)\n",
    "\n",
    "# Printing out number of tokens available\n",
    "print(\"Number of German Tokens: {}\".format(len(de_words)))\n",
    "\n",
    "zh_words = []\n",
    "for word in zh_model.vocab:\n",
    "    zh_words.append(word)\n",
    "\n",
    "# Printing out number of tokens available\n",
    "print(\"Number of Chinese Tokens: {}\".format(len(zh_words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditionalToSimplified(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        file[i] = ''.join(HanziConv.toSimplified(el))\n",
    "        i += 1\n",
    "        \n",
    "def chineseTokenize(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        file[i] = ' '.join(jieba.cut(el, cut_all=False, HMM=True))\n",
    "        i += 1\n",
    "\n",
    "        \n",
    "from nltk.tokenize import MWETokenizer\n",
    "def MWEtokenize(el):\n",
    "    i = 0\n",
    "    tokenizer = MWETokenizer(('barack','obama'))\n",
    "    tokenizer.add_mwe([('new','york'),('hong', 'kong'), ('los', 'angeles'), ('san', 'francisco'),('united', 'kingdom')])\n",
    "    el = tokenizer.tokenize(el.split())\n",
    "    return el\n",
    "        \n",
    "def tokenize(file):\n",
    "    i = 0\n",
    "    for el in file:\n",
    "        tokenizerOne = WordPunctTokenizer()\n",
    "        el = tokenizerOne.tokenize(str(el.lower()))\n",
    "        el = ' '.join([word for word in el if word not in punctuation])\n",
    "        file[i] = MWEtokenize(el)\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "punctuation = punctuation + str('；')+  str(\"：《》「 」“”[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\")+str('编辑')+str('%.[')+str('(/')+str(');[')+str('\"),')+str(').')+str('.[')+str(',[')+str('][')+str('(\"')+str('.\"[')+str('—'+str('.\"'))+str('.,')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document-vector building function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input: \n",
    "    #1. list of preprocessing documents\n",
    "    #2. language model\n",
    "    #3. entity name\n",
    "    #4. language: En/De\n",
    "    #5. representation of aspects: headline/content\n",
    "## output: for each documnet, a document vector will be produced\n",
    "\n",
    "def getDocVectors(content, language_model, termname ,language, aspect):\n",
    "    \n",
    "    language_model = language_model\n",
    "\n",
    "    words = []\n",
    "    for word in language_model.vocab:\n",
    "        words.append(word)\n",
    "\n",
    "    doc_vectors = list()\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        #print(content[i])\n",
    "        vectorSum = [0.0000]*300\n",
    "        l = 0\n",
    "        for el in content[i]:\n",
    "            if el in words:\n",
    "                #print(list(vectors[el]))\n",
    "                vectorSum = list(map(add, list(language_model[el]), vectorSum))\n",
    "                l+=1\n",
    "            #else:\n",
    "                #print(el)\n",
    "        #print(vectorSum)\n",
    "        for m in range(len(vectorSum)):\n",
    "            if vectorSum[m] != 0:\n",
    "                vectorSum[m] = float(vectorSum[m])/l ###average the vector sum\n",
    "            else:\n",
    "                vectorSum[m] = vectorSum[m]\n",
    "\n",
    "        doc_vectors.append(vectorSum)\n",
    "    \n",
    "    doc_vectors_final = []\n",
    "    for vec in doc_vectors:\n",
    "        doc = []\n",
    "        for dim in vec:\n",
    "            \n",
    "            doc.append(float(dim))\n",
    "        \n",
    "        doc_vectors_final.append(doc)\n",
    "            \n",
    "        \n",
    "    with open(language+'_'+termname+'_'+aspect+\".json\", 'w') as f:\n",
    "        json.dump(doc_vectors_final, f)\n",
    "  \n",
    "    \n",
    "    #return doc_vectors_final \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the function! get and save the document vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/hahou/WikiDataCrawling/English Corpus/source_en_Bracak_Obama.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-480ded2ae989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentity_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# loading source data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/hahou/WikiDataCrawling/English Corpus/source_en_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0msource_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/hahou/WikiDataCrawling/German Corpus/source_de_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/hahou/WikiDataCrawling/English Corpus/source_en_Bracak_Obama.json'"
     ]
    }
   ],
   "source": [
    "# define the entity list\n",
    "#entity_list = ['United_Kingdom', 'Italy', 'Asia','Europe']\n",
    "\n",
    "#entity_list =  ['Russia','singapore','India', 'Israel','Brazil','Philippines', 'New_York_City',\n",
    "#'London','Singapore','Hong_Kong','Dubai','Los_Angeles','Paris','Chicago','Washington,_D.C.','San_Francisco',\n",
    "#'Mumbai','Rome','Toronto','Philadelphia','Monaco','Tokyo','Amsterdam','Boston','Barcelona','Peking']\n",
    "\n",
    "entity_list = ['Bracak_Obama', 'Donald_Trump']\n",
    "\n",
    "for entity in entity_list:\n",
    "    # loading source data\n",
    "    with open('/home/hahou/WikiDataCrawling/English Corpus/source_en_'+entity+'.json') as json_data:\n",
    "        source_en = json.load(json_data)\n",
    "    with open('/home/hahou/WikiDataCrawling/German Corpus/source_de_'+entity+'.json') as json_data:\n",
    "        source_de = json.load(json_data)\n",
    "    #with open('/home/hahou/WikiDataCrawling/Chinese Corpus/source_de_'+entity+'.json') as json_data:\n",
    "        #source_zh = json.load(json_data)    \n",
    "    \n",
    "    # exact headlies\n",
    "    en_headline = list(source_en.keys())\n",
    "    de_headline = list(source_de.keys())\n",
    "    zh_headline = list(source_zh.keys())\n",
    "\n",
    "    #extract context\n",
    "    en_content = []\n",
    "    de_content = []\n",
    "    zh_content = []\n",
    "    for el in en_headline:\n",
    "        en_content.append(''.join(source_en[el]))\n",
    "    for el in de_headline:\n",
    "        de_content.append(''.join(source_de[el]))\n",
    "    for el in zh_headline:\n",
    "        zh_content.append(''.join(source_de[el]))\n",
    "        \n",
    "    # preprocessing for en and de\n",
    "    tokenize(en_content)\n",
    "    tokenize(de_content)\n",
    "    tokenize(en_headline)\n",
    "    tokenize(de_headline)\n",
    "    \n",
    "    \n",
    "    #preprocessing for zh\n",
    "    traditionalToSimplified(zh_content)\n",
    "    chineseTokenize(zh_content)\n",
    "    removePunctuation(zh_content)\n",
    "\n",
    "    traditionalToSimplified(zh_headline)\n",
    "    chineseTokenize(zh_headline)\n",
    "    removePunctuation(zh_headline)    \n",
    "    \n",
    "    \n",
    "    # get the document vectors for headlines of entry page section\n",
    "    getDocVectors(en_headline, en_model, entity,'En', 'Headline')\n",
    "    getDocVectors(de_headline, de_model, entity,'De', 'Headline')\n",
    "    getDocVectors(zh_headline, zh_model, entity,'zh', 'Headline')\n",
    "    \n",
    "    # get the document vectors for Contents of entry page section\n",
    "    getDocVectors(en_content, en_model, entity,'En', 'Content')\n",
    "    getDocVectors(de_content, de_model, entity,'De', 'Content')\n",
    "    getDocVectors(zh_content, zh_model, entity,'zh', 'Content')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run functions seperately for each entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading source data\n",
    "with open('/home/hahou/WikiDataCrawling/English Corpus/source_en_United_States.json') as json_data:\n",
    "    source_en = json.load(json_data)\n",
    "\n",
    "with open('/home/hahou/WikiDataCrawling/German Corpus/source_de_United_States.json') as json_data:\n",
    "    source_de = json.load(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact headlies\n",
    "en_headline = list(source_en.keys())\n",
    "de_headline = list(source_de.keys())\n",
    "\n",
    "#extract context\n",
    "en_content = []\n",
    "de_content = []\n",
    "for el in en_headline:\n",
    "    en_content.append(''.join(source_en[el]))\n",
    "for el in de_headline:\n",
    "    de_content.append(''.join(source_de[el]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizetion\n",
    "tokenize(en_content)\n",
    "tokenize(de_content)\n",
    "\n",
    "tokenize(en_headline)\n",
    "tokenize(de_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['united_states'],\n",
       " ['etymology'],\n",
       " ['history'],\n",
       " ['geography', 'climate', 'and', 'environment'],\n",
       " ['demographics'],\n",
       " ['government', 'and', 'politics'],\n",
       " ['law', 'enforcement', 'and', 'crime'],\n",
       " ['economy'],\n",
       " ['infrastructure'],\n",
       " ['culture']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_content = convert2Common_texts(en_content)\\nde_content = convert2Common_texts(de_content)\\nen_content = toLowerCase(en_content)\\nde_content = toLowerCase(de_content)\\n\\nen_headline = convert2Common_texts(en_headline)\\nde_headline = convert2Common_texts(de_headline)\\nen_headline = toLowerCase(en_headline)\\nde_headline = toLowerCase(de_headline)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''en_content = convert2Common_texts(en_content)\n",
    "de_content = convert2Common_texts(de_content)\n",
    "en_content = toLowerCase(en_content)\n",
    "de_content = toLowerCase(de_content)\n",
    "\n",
    "en_headline = convert2Common_texts(en_headline)\n",
    "de_headline = convert2Common_texts(de_headline)\n",
    "en_headline = toLowerCase(en_headline)\n",
    "de_headline = toLowerCase(de_headline)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: 1* n dims sparse matrix, or single vector from query_vect/ train_tfidf matrix\n",
    "def getSquareSum(vector):\n",
    "    squaresum = 0\n",
    "    for i in range(vector.shape[0]):\n",
    "        squaresum += vector[i]* vector[i]\n",
    "        \n",
    "    return squaresum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: two 1* n sparse matrixs, or single vector from query_vect/ train_tfidf matrixs\n",
    "# return the cosine sim of two vectors, type: float\n",
    "\n",
    "def getCosineSimilarity(query_vector, doc_vector):\n",
    "    squaresum_query = getSquareSum(query_vector)\n",
    "    squaresum_doc= getSquareSum(doc_vector)\n",
    "    if math.sqrt(squaresum_query)*math.sqrt(squaresum_doc) > 0:\n",
    "        sim = np.dot(query_vector, doc_vector)/(math.sqrt(squaresum_query)*math.sqrt(squaresum_doc))\n",
    "        return sim\n",
    "    else:\n",
    "        return np.dot(query_vector, doc_vector)\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the document vectors for headlines of entry page section\n",
    "getDocVectors(en_headline, en_model, 'United_States','En', 'Headline')\n",
    "getDocVectors(de_headline, de_model, 'United_States','De', 'Headline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the document vectors for Contents of entry page section\n",
    "getDocVectors(en_content, en_model, 'United_States','En', 'Content')\n",
    "getDocVectors(de_content, de_model, 'United_States','De', 'Content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def saveDocVector(DV, termname, language, aspect):\n",
    "\n",
    "    with open( language+termname+aspect+\".json\", 'w') as f:\n",
    "        json.dump(DV, f)\n",
    "        \n",
    "def saveDocVector_headline_De(DV, termname):\n",
    "\n",
    "    with open( 'De_model_headline'+termname+\".json\", 'w') as f:\n",
    "        json.dump(DV, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDocVector_En(DV_en_content, 'United_States', 'En', 'Headline')\n",
    "saveDocVector_De(DV_de_content, 'United_States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getCosineSimilarity(DV_en_content[0],DV_de_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input:\n",
    "# assume we want to link wiki-text in deutsch(arrow language) with the wiki-text in english(target):\n",
    "# src_content: is the source page data of the wikipedia item in laguage a\n",
    "# trs_content: is the translation data of the wikipedia item from language b to language a\n",
    "# en_model: english language model\n",
    "# de_model: deutsch language\n",
    "# arrow_la: arrow language of the wiki-text\n",
    "# target_la: target language of the wiki-text\n",
    "# termname: entry name of the wiki-text\n",
    "\n",
    "## return: RUN file for Trec_eval\n",
    "\n",
    "def getWordEmbeddingRun(en_content, de_content, DV_en_content, DV_de_content, target_la, arrow_la, termname):    \n",
    "    \n",
    "    ### get the similarity of query with each doc leader\n",
    "    \n",
    "    #get document vector for each aspect\n",
    "    DV_en_content = DV_en_content\n",
    "    DV_de_content = DV_de_content\n",
    "    \n",
    "    \n",
    "    result = []\n",
    "    for j in range(len(DV_de_content)):\n",
    "        print (DV_de_content[j][:10])\n",
    "        \n",
    "        sims = []\n",
    "        results_ID = []\n",
    "        results = []\n",
    "        for i in range(len(DV_en_content)):\n",
    "            s = getCosineSimilarity(DV_de_content[j], DV_en_content[i])\n",
    "            #if type(s)is not None:\n",
    "            print(s)\n",
    "            sims.append(s)\n",
    "        \n",
    "        for x in range(len(sims)):\n",
    "            if sims[x] >0:\n",
    "                results_ID.append(x)\n",
    "                results.append(sims[x])\n",
    "        \n",
    "        #print(results, results_ID)\n",
    "        \n",
    "        if len(results) >0 and len(results_ID)>0:\n",
    "            results, results_ID= zip(*sorted(zip(results, results_ID), reverse=True))      \n",
    "       \n",
    "        \n",
    "        for m in range(len(results)):\n",
    "            result.append([arrow_la+'_'+termname+\"_h\"+str(j), 0, target_la+'_'+termname+'_h'+str(results_ID[m]), m, results[m], 'wordembedding'])\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    if len(de_content[0]) > 10 and len(en_content[0])>10 :\n",
    "        df.to_csv(termname+'_'+arrow_la+'_'+target_la+'_text_WE.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    elif len(de_content[0]) < 10 and len(en_content[0])<10:\n",
    "        df.to_csv(termname+'_'+arrow_la+'_'+target_la+'_headline_WE.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    elif len(de_content[0]) < 10 and len(en_content[0])>10:\n",
    "        df.to_csv(termname+'_'+arrow_la+'_'+target_la+'_headline_text_WE.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    else:\n",
    "        df.to_csv(termname+'_'+arrow_la+'_'+target_la+'_text_headline_WE.txt', header=None, index=None, sep=' ', mode='a')     \n",
    "    \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# do not run\n",
    "#DV_en_content = getDocVectors(en_content, en_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DV_de_content = getDocVectors(de_content, de_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getWordEmbeddingRun(en_content, de_headline, DV_en_content, DV_de_content,  'en', 'de', 'China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getWordEmbeddingRun(en_content, de_content, DV_en_content, DV_de_content, 'en', 'de', 'China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getWordEmbeddingRun(en_headline, de_headline, DV_en_content, DV_de_content, 'en', 'de', 'China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getWordEmbeddingRun(en_headline, de_content, DV_en_content, DV_de_content, 'en', 'de', 'China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trs_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
